---
layout: post
title: Python cookbook Chapter 2 한글
tags: ['python', 'docs', 'string', '한글']
progress: 45
---

<div class='warn'>
이 문서는 Python Coobook 3rd edition - O'REILLY, David Beazley & Brian K. jones 를 참고한 것이며 <a href="//wikidocs.net/book/1">Python을 완전히 처음 접하는 경우</a>에는 적합하지 않습니다.<br>개인적으로 공부한 내용이라 오역이 있을 수 있으며 <strong>Memo</strong>에는 실제 C를 이용한 테스트를 하지 않았으므로 이해를 위한 추측성 부분이 많음에 유의하시기 바랍니다.<br>
</div>

거의 모든 유용한 프로그램은 데이터를 파싱하던지 또는 출력을 생성하는 일종의 텍스트 처리를 포함합니다. 이 챕터는 문자열 분리, searching, substitution, lexing, parsing과 같은 텍스트 조작을 포함하는 일반적인 문제에 초점을 맞춥니다. 이러한 많은 작업은 `string`의 `built-in method`를 사용하여 쉽게 해결할 수 있습니다. 하지만 좀더 복잡한 연산을 수행하려면 `regular expressions`를 사용하거나 본격적인 `parser`를 만들어야 할 수 있습니다. 이 모든 주제들을 담았습니다. 또한 유니코드로 작업할 때 몇가지 까다로운 부분을 다룹니다.

## 2.1 복합적인 Delimiter에서 문자열 분리

#### Problem

필드에서 문자열 분리를 하고 싶은데 `delimiter(구분 기호)` (및 그 주위의 `space`)가 문자열 전체에서 일관성이 없습니다.

#### Solution

`string` 객체에서의 `split()` 메서드는 매우 단순한 경우를 의미하고, 복합적인 `delimiter`를 허용하지 않거나 `delimiter` 주위의 가능한 `whitespace`를 고려하지 못합니다. 좀더 유연할 필요가 있을 때는 `re.split()` 메서드를 사용합니다.

```python
line = 'asdf fjdk; afed, fjek,asdf,         foo'

import re
print(re.split(r'[;,\s]\s*', line))        # ['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']
```

`re.split()` 함수는 분리 기호에 복합적인 패턴을 지정할 수 있기 때문에 유용합니다. 예를 들어 위의 solution에서 보듯이 분리 기호는 (,) (;) 또는 `whitespace(\s)` 뒤에 임의의 추가적인 `whitespace` 입니다. 패턴이 발견될 때마다, 전체적으로 매치되는 부분은 매치된 양 쪽에 있는 필드 사이에서 `delimiter`가 됩니다. 결과는 `str.split()`과 같이 필드들의 `list`를 반환합니다.

`re.split()`을 사용할 때는 괄호 안에 `capture group`을 포함하는 `regular expression` 패턴을 약간 조심해야 할 필요가 있습니다. 그 `capture group` 사용한다면 매치된 텍스트도 결과에 포함됩니다.

```python
fields = re.split(r'(;|,|\s)\s*', line)
print(fields)   # ['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo']
```

분리된 `character`를 얻는 것은 특정 상황에 유용할 수 있습니다. 예를 들어 나중에 출력 문자열을 수정하려면 `split character`가 필요할 수 있습니다.

```python
values = fields[::2]
delimiters = fields[1::2] + ['']
print(values)       # ['asdf, 'fjdk', 'afed', 'fjek', 'asdf', 'foo']
print(delimiters)   # [' ', ';', ',', ',', ',', '']

# Reform the line using the same delimiters
print(''.join(v+d for v, d in zip(values, delimiters))) # 'asdf fjdk;afed,fjek,asdf,foo'
```

결과의 구분 기호를 사용하고 싶지 않지만 여전히 괄호를 사용하여 `regular expression` 패턴의 그룹을 사용하고자 한다면 `(?:...)`로 지정되는 `noncapture group`을 사용하면 됩니다.

```python
re.split(r'(?:,|;|\s)\s*', line)
```

#### Memo

문자열 앞의 `r`은 그 문자열이 `raw string`이라는 것을 알려주는 것입니다.

백슬래시 충돌 문제를 이 `r`을 통해 해결할 수 있습니다.

[참고](//wikidocs.net/4308)

## 2.2 문자열의 처음과 끝에 매칭되는 텍스트

#### Problem

파일 이름이나 확장자, `URL scheme` 등 특정 텍스트 패턴의 문자열의 시작 또는 끝을 체크할 필요가 있습니다.

#### Solution

문자열의 시작 또는 끝을 체크하는 간단한 방법은 `str.startswith()` 또는 `str.endswith()` 메서드를 사용하는 것입니다.

```python
filename = 'spam.txt'
filename.endswith('.txt')       # True
filename.startswith('file:')    # False
url = 'http://www.python.org'
url.startswith('http:')         # True
```

여러 선택에 대해 체크해야 하는 경우 간단히 `startswith()` 또는 `endswith()`에 가능한 `tuple`을 제공하면 됩니다.

```python
import os
filenames = os.listdir('.')
# filenames = ['Makefile', 'foo.c', 'bar.py', 'spam.c', 'spam.h']
print([name for name in filenames if name.endswith(('.c', '.h'))])
print(any(name.endswith('.py') for name in filenames))      # True
```

여기 다른 예시도 있습니다.

```python
from urllib.request import urlopen

def read_data(name):
    if name.startswith(('http:', 'https:', 'ftp:')):
        return urlopen(name).read()
    else:
        with open(name) as f:
            return f.read()
```

이상하게도 `tuple`이 실제로 `input`으로 요구됩니다. `list`나 `set`으로 지정된 선택을 했다면 `tuple`을 먼저 사용함으로써 변환해 주어야 합니다.

```python
choices = ['http:', 'ftp:']
url = 'http://www.python.org'
url.startswith(choices)         # TypeError startswith first arg must be str or a tuple of str, not list
url.startswith(tuple(choices))  # True
```

#### Discussion

`startswith()`와 `endswith()` 메서드는 기본적인 `prefix`와 `suffix` 검사를 수행하기 위한 매우 편리한 방법을 제공합니다. `slice`를 써서 비슷한 작업이 수행될 수 있지만, 조금 아름다운 코드는 되지 못합니다.

```python
filename = 'spam.txt'
filename[-4:] == '.txt'         # True
url = 'http://www.python.org'
url[:5] == 'http:' or url[:6] == 'https:' or url[:4] == 'ftp:'  # True
```

또한 `regular expression`로 사용하려는 경향이 있을 경우가 있습니다.

```python
import re
url = 'http://python.org'
re.match('http:|https:|ftp:', url)
```

이 역시 동작하지만, 종종 단순한 매치에는 과도합니다. solution을 사용하는 것이 좀 더 간단하고 빠르게 실행됩니다.

마지막으로 하지만 적어도, `startswith()`와 `endswith()` 메서드가 다른 일반적인 `data reduction`같은 `operation`과 결합 될 때 보기 좋습니다. 예를 들어 특정 종류의 파일이 있는지 디렉터리를 체크하는 `statement`는 다음과 같습니다.

```python
if any(name.endswith(('.c', '.h')) for name in listdir(dirname)):
    pass
```

## 2.3 Shell Wildcard 패턴을 사용하여 문자열 매칭

#### Problem

`Unix shell`에서 동작할 때 일반적으로 사용되는 `wildcard`(`*.py,Dat[0-9]*.csv`와 같은) 패턴과 동일한 사용으로 텍스트를 매칭하고 싶습니다.

#### Solution

`fnmatch` 모듈은 어떤 매칭을 수행하는데 사용되는 두 함수 `fnmatch()`와 `fnmatchcase()`를 제공합니다. 사용은 간단합니다.

```python
from fnmatch import fnmatch, fnmatchcase
fnmatch('foo.txt', '*.txt')         # True
fnmatch('foo.txt', '?oo.txt')       # True
fnmatch('Dat45.csv', 'Dat[0-9]*')   # True

names = ['Dat1.csv', 'Dat2.csv', 'config.ini', 'foo.py']
print([name for name in names if fnmatch(name, 'Dat*.csv')])
# ['Dat1.csv', 'Dat2.csv']
```

일반적으로 `fnmatch()`는 파일 시스템 아래의(다양한 운영체제에 따라 다른) 동일한 대소문자를 구분하는데 사용하여 패턴을 매치합니다.

```python
# On OS X (Mac)
fnmatch('foo.txt', '*.TXT')     # False

# On Windows
fnmatch('foo.txt', '*.TXT')     # True
```

이런 구별이 중요해질 경우에는 대신 `fnmatchcase()`를 사용하시면 됩니다. 이는 제공한 대 소문자 규칙을 기반으로 정확하게 매치됩니다.

```python
from fnmatch import fnmatchcase
fnmatchcase('foo.txt', '*.TXT') # False
```

이런 함수의 `overlooked feature`(포괄적인 기능)은 파일 이름이 아닌 문자열의 데이터 처리와 함께 잠재적으로 사용되는 것입니다.
예를 들어 다음과 같이 주소의 `list`를 가지고 있다고 하면

```python
addresses = [
    '5412 N CLARK ST',
    '1060 W ADDISON ST',
    '1039 W GRANVILLE AVE',
    '2122 N CLARK ST',
    '4802 N BROADWAY'
]
```

`list comprehension`을 다음과 같이 쓸 수 있습니다.

```python
from fnmatch import fnmatchcase
print([addr for addr in addresses if fnmatchcase(addr, * ST')])
# ['5412 N CLARK ST', '1060 W ADDISON ST', '2122 N CLARK ST']
print([addr for addr in addresses if fnmatchcase(addr, '54[0-9][0-9] *CLARK*')])
# ['5412 N CLARK ST']
```

#### Discussion

`fnmatch`로 수행된 매칭은 간단한 문자열 메서드의 기능과 `regular expression`의 모든 기능 사이의 어디에서나 위치합니다. 단지 데이터 처리 연산의 `wildcard`를 허용하기 위한 간단한 메커니즘을 제공하려면 종종 합리적인 해결책이 될 수 있습니다.

만약 실제 파일 이름을 매치하는 코드를 쓰려고 한다면 [5.13장]()의 `glob` 모듈을 대싱 사용하시면 됩니다.

## 2.4 텍스트 패턴을 위한 Matching과 Searching

#### Problem

특정 패턴을 위한 텍스트를 매치하거나 찾고 싶습니다.

#### Solution

텍스트를 간단히 글자 그대로 매치하려고 한다면, 단지 `str.find()`, `str.endswith()`, `str.startswith()`같은 기본적인 `string` 메서드를 사용할 수 있습니다. 

```python
text = 'yeah, but no, but yeah, but no, but yeah'

# Exact match
print(text == 'yeah')           # False

# Match at start or end
print(text.startswith('yeah'))  # True
print(text.endswith('no'))      # False

# Search for the location of the first occurrence
text.find('no')                 # 10
```

좀 더 복잡한 매칭을 위해서는 `re` 모듈과 `regular expression`을 사용하면 됩니다. `regular expression`의 기본적인 메커니즘을 설명하기 위해 "11/27/2012" 같이 날짜를 지정된 숫자에 매치하길 원한다고 하면

```python
text1 = '11/27/2012'
text2 = 'Nov 27, 2012'

import re
# Simple matching: \d+ means match one or more digits
def textmatch(text)
    if re.match(r'\d+/\d+/\d+', text):
        print('yes')
    else:
        print('no')

textmatch(text1)        # yes
textmatch(text2)        # no
```

같은 패턴을 사용하여 많은 양의 매칭을 수행하려고 하면 `regular expression` 패턴을 먼저 패턴 객체로 `precompile`하는 것이 일반적인 방법입니다.

```python
datepat = re.compile(r'\d+/\d+/\d+')

def textmatch(text):
    if datepat.match(text):
        print('yes')
    else:
        print('no')

textmatch(text1)        # yes
textmatch(text2)        # no
```

`match()`는 항상 문자열의 시작부분에서 일치하는 부분을 찾으려고 합니다. 패턴이 일어나는 모든 부분의 텍스트를 찾으려면 `findall()` 메서드를 대신 사용하면 됩니다.

```python
text = 'Todays is 11/27/2012. PyCon starts 3/13/2013.'
print(datepat.findall(text)) # ['11/27/2012', '3/13/2013']
```

`regular expression`을 정의할 때는 괄호 안의 패턴의 일부분을 묶어 `capture group`을 도입하는 것이 일반적입니다.

```python
datepat = re.compile(r'(\d+)/(\d+)/(\d+)')
```

`capture group`들은 각 그룹의 내용을 개별적으로 추출할 수 있기 때문에 매치된 텍스트의 후속 처리를 단순화하는 것입니다.

```python
m = datapat.match('11/27/2012')

# Extract the contents of each group
print(m.group(0))       # '11/27/2012'
print(m.group(1))       # 11
print(m.group(2))       # 27
print(m.group(3))       # 2012
print(m.groups())       # ('11', '27', '2012')

# Find all matches (notice splitting into tuples)
text = 'Today is 11/27/2012. PyCon starts 3/13/2013'
datepat.findall(text)   # [('11', '27', '2012'), ('3', '13', '2013')]

for month, day, year in datepat.findall(text):
    print('{}-{}-{}'.format(year, month, day))
```

`findall()` 메서드는 텍스트를 검색하고 모든 매치를 찾아 그 리스트를 반환합니다. 매치를 순회하면서 찾으려면 `finditer()` 메서드를 사용하시면 됩니다.

```python
for m in datepat.finditer(text):
    print(m.groups())
```

#### Discussion

`regular expression`의 이론에 대한 기본적인 튜토리얼은 이 책의 범위를 벗어납니다. 하지만 위에서 소개한 방법은 `re` 모듈을 사용하여 텍스트를 검색하고 매칭하는 데 절대적인 기본 사항을 설명합니다. 필수적인 기능은 `re.compile()`을 사용하여 처음 패턴을 컴파일 하고 `match()`, `findall()`, 또는 `finditer()`같은 메서드를 사용하는 것입니다.

패턴을 지정할 때는 `'r(\d+)/(\d+)/(\d+)'`와 같은 `raw string`을 사용하는 것이 상대적으로 일반적입니다. 이런 문자열은 백슬래시 문자를 해석되지 않고 남기며 `regular expression`의 내용에 유용할 수 있습니다. 그렇지 않으면 이중 백슬래시를 `'(\\d+)/(\\d+)/(\\d+)'`와 같이 사용해야 합니다.

`match()` 메서드가 문자열의 시작 부분 부터만 체크한다는 것에 주의하셔야 합니다. 원하지 않는 매칭이 일어날 수 있습니다.

```python
# 앞 부분 부터 맞기만 하면 됨 (정확한 매칭은 아님)
m = datepat.match('11/27/2012abcdef')
print(m.group())        # '11/27/2012'
```

정확한 매칭을 하려면 패턴에 `end-marker($)`를 포함해야 합니다.

```python
datepat = re.compile(r'(\d+)/(\d+)/(\d+)$')
print(datepat.match('11/27/2012abcdef'))        # None
print(datepat.match('11/27/2012'))              # <_sre.SRE_Match object; span=(0, 10), match='11/27/2012'>
```

마지막으로 단순하게 텍스트 매칭/검색 만 수행하려면 컴파일 단계를 건너 뛰고 `re` 모듈의 모듈 레벨 함수를 대신 사용하는 방법이 있습니다.

```python
re.findall(r'(\d+)/(\d+)/(\d+)', text)           # [('11', '27', '2012'), ('3', '13', '2013')]
```

그러나 매칭이나 검색을 많이 수행할 경우에는, 보통 패턴을 먼저 컴파일 하고 계속해서 사용하는 것이 좋습니다. 모듈 레벨의 함수는 최근에 컴파일 된 패턴의 캐시를 유지하므로, 성능에 크게 영향을 주진 않지만 지신이 직접 컴파일 한 패턴을 사용함으로써 추가적인 처리와 몇가지 조회를 줄일 수는 있습니다.


## 2.5 Searching and Replacing Text

#### Problem

문자열의 텍스트패턴을 검색하고 대체하고 싶습니다.

#### Solution

글자 그대로의 간단한 패턴에서는 `str.replace()`메서드를 사용하면 됩니다.

```python
text = 'yeah, but no, but yeah, but no, but yeah'
text.replace('yeah', 'yep')    # 'yep, but no, but yep, but no, but yep'
```

좀 더 복잡한 패턴에서는 `re` 모듈의 `sub()` 함수/메서드를 사용하면 됩니다. "11/27/2012"를 "2012-11-27"로 다시 쓰고 싶다면 간단하게 다음과 같이 하면 됩니다.

```python
text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'
import re
re.sub(r'(\d+)/(\d+)/(\d+)', r'\3-\1-\2', text)   # 'Today is 2012-11-27. PyCon starts 2013-3-13.'
```

`sub()`의 첫 번째 인수는 매치할 패턴이고 두 번째 인수는 대체할 패턴입니다. `\3'과 같이 백슬래시된 숫자는 패턴에서의 `capture group` 숫자를 가리킵니다.

만약 같은 패턴에 대한 반복된 대체를 수행하려면 먼저 컴파일을 고려하는 것이 좋은 성능을 낼 수 있습니다.

```python
import re
datepat = re.compile(r'(\d+)/(\d+)/(\d+)')
datepat.sub(r'\3-\1-\2', text)
```

좀 더 복잡한 대체를 하려면 대체 `callback` 함수를 지정할 수 있습니다.

```python
from calendar import month_abbr
def change_date(m):
    mon_name = month_abbr[int(m.group(1))]
    return '{} {} {}'.format(m.group(2), mon_name, m.group(3))

datepat.sub(change_date, text)    # 'Today is 27 Nov 2012. PyCon Starts 13 Mar 2013.'
```

입력에서, 대체 `callback`에 대한 인수는 `match()` 또는 `find()` 객체로 반환된 매치된 객체입니다. 매치에서 지정된 부분을 추출하기 위해서는 `group()` 메서드를 사용합니다. 그 함수는 바뀐 텍스트를 반환하게 됩니다.

대체된 텍스트를 얻는 것에 추가로 얼마나 많이 대체되었는지 알고 싶을 때는 `re.subn()`을 대신 사용합니다.

```python
newext, n = datepat.subn(r'\3-\1-\2', text)
print(newtext)    # 'Today is 2012-11-27. PyCon starts 2013-3-13.'
print(n)          # 2
```

#### Discussion

`regular expression` 검색 및 대체에 대해 위의 `sub()` 메서드 보다 많은 것은 없습니다. 가장 까다로운 부분은 `regular expression` 패턴을 지정하는 것입니다. 이 부분은 독자에게 연습 문제로 남기는 것이 가장 좋습니다.

#### Memo

`regular expression`에 대한 부분

- [perl regular expression](//www.perl.or.kr/perl_iyagi/regexp)

- [regular expression visualization](//regexper.com)

- [regular expression book](//shop.oreilly.com/product/9780596528126.do)

## 2.6 Searching and Replacing Case-Insensitive Text

#### Problem

대소문자를 구분하지 않고 텍스트를 검색하고 대체할 필요가 있습니다.

#### Solution

대소문자를 구분하지 않는 텍스트 작업을 수행하려면 `re` 모듈을 사용하고 다양한 작업을 위해 `re.IGNORECASE` 플래그를 제공해야 합니다.

```python
text = 'UPPER PYTHON, lower python, Mixed Python'
print(re.findall('python', text, flags=re.IGNORECASE))        # ['PYTHON', 'python', 'Python']
print(re.sub('python', 'snake', text, flags=re.IGNORECASE))   # 'UPPER snake, lower snake, Mixed snake'
```

마지막 예시는 텍스트 대체가 매치된 텍스트로 대소문자 변환이 되지 않는다는 한계를 보여줍니다.
이를 고치고 싶으면 다음과 같은 지원 함수를 사용해야 할 수 있습니다.

```python
def matchcase(word):
    def replace(m):
        text = m.group()
        if text.isupper():
            return word.upper()
        elif text.islower():
            return word.lower()
        elif text[0].isupper():
            return word.capitalize()
        else:
            return word
    return replace

re.sub('python', matchcase('snake'), text, flags=re.IGNORECASE)  # 'UPPER SNAKE, lower snake, Mixed Snake'
```

#### Discussion

간단한 경우에는 `re.IGNORECASE`를 제공함으로써 충분히 대소문자를 구분하지 않는 매칭을 수행할 수 있습니다. 하지만 대소문자 `folding`과 관련된 유니코드 매칭에는 충분하지 않을 수 있음에 주의하셔야 합니다. [2.10장]()에 자세한 내용이 있습니다.

## 2.7 가장 짧은 매치를 위한 Regular Expression 지정하기

#### Problem

'regular expression`을 사용하여 텍스트 패턴을 일치시키려 하지만 가장 긴 패턴만 식별합니다. 대신에 가장 짧은 매치를 찾게 바꾸려고 합니다.

#### Solution

인용 부호 같이 시작과 끝의 `delimiter` 쌍 안쪽 부분의 텍스트를 매칭하려 하는 패턴에서 이 문제가 종종 일어납니다. 

```python
str_pat = re.compile(r'\"(.*)\"')
text1 = 'Computer says "no."'
print(str_pat.findall(text1))    # ['no.']
text2 = 'Computer says "no.", Phone says "yes."'
print(str_pat.findall(text2))    # ['no." Phone says "yes.']
```

이 예제에서는 `r'\"(.*)\"'` 패턴이 인용 부호로 묶인 텍스트를 찾으려고 합니다. 하지만 `regular expression`의 `* operator`가 욕심이 많아 매칭은 가장 긴 매치를 찾습니다. 그러므로 `text2`를 포함하는 두번째 예제에서는 두 인용부호 문자열에서 매치가 부정확합니다.

이를 고치려면 패턴에서 `? modifier`를 `* operator` 뒤에 추가합니다. 

```python
str_pat = re.compile(r'\"(.*?)\"')
print(str_pat.findall(text2))    # ['no.', 'yes.']
```

이는 매칭의 욕심을 없애고 대신 가장 짧은 매치로 만들어 줍니다.

#### Discussion

이 방법은 `(.) character` 사용을 포함하는 `regular expression` 하나 이상의 일반적인 문제를 다룹니다. 패턴에서 점은 개행(newline)을 제외한 모든 `character`를 매치합니다. 반면에 인용 부호 같이 시작과 끝 텍스트로 점을 괄호로 묶는 경우에는 패턴 매치에 가능한 가장 긴 매치를 찾으려고 합니다. 이로 인해 시작 또는 끝 텍스트가 여러번 건너뛰고 가장 길게 매치된 결과를 포함하게 됩니다. `?`를 `*` 또는 `+ operator` 오른쪽에 추가하는 것은 매칭 알고리즘이 가능한 가장 짧은 매치를 대신 찾게끔 합니다. 

## 2.8 여러 라인의 패턴을 위한 regular expression` 쓰기

#### Problem

`regular expression`을 사용하여 텍스트의 블록을 매치하려고 합니다. 하지만 여러줄에 걸친 매치가 필요합니다.

#### Solution

이 문제는 점을 사용하는 패턴에서 일반적으로 일어나지만 줄바꿈을 매치하지는 않는 사실을 잊지 말아야 합니다. 다음과 같이 C 스타일의 주석을 매치하려고 하면

```python
comment = re.compile(r'/\*(.*?)\*/')
text1 = '/* this is a comment */'
text2 = '''/* this is a
multiline comment */
'''

print(comment.findall(text1))    # [' this is a comment ']
print(comment.findall(text2))    # []
```

이 문제를 해결하려면 줄바꿈을 위한 지원을 추가할 수 있습니다.

```python
comment = re.compile(r'/\*((?:.|\n)\*/')
print(comment.findall(text2))    # [' this is a\n multiline comment ']
```

이 패턴에서, (?:.|\n)은 `noncapture group`을 지정합니다. (즉 이 그룹은 매칭할 그룹을 정의하지만, 해당 그룹은 개별적으로 캡쳐되거나 번호가 매겨지지는 않습니다.)

#### Discussion

`re.compile()` 함수는 여기서 유용한 `re.DOTALL` 플래그를 받습니다. 이는 `regular expression`의 `.`을 줄바꿈을 포함한 모든 캐릭터에 매치시킵니다.

```python
comment = re.compile(r'/\*(.*?)\*/', re.DOTALL)
print(comment.findall(text2))    # [' this is a\n multiline comment ']
```

`re.DOTALL` 플래그를 사용함으로써 간단한 케이스에 대해 잘 작동하지만 매우 복잡한 패턴 을 사용하거나 [2.18장]()에 소개된 것과 같이 `tokenizing`의 목적을 위해 함께 사용하게 되는 별도의 `regular expression`을 혼합하여 사용하는 경우에는 문제가 될 수 있습니다.
선택이 주어진다면 여분의 플래그가 필요 없이 올바르게 작동하도록 `regular expression` 패턴을 정의하는 것이 좋습니다.

## 2.9 Normalizing Unicode Text to a Standard Representation

#### Problem

유니코드 문자열로 작업하지만 모든 문자열이 동일한 기본 표현을 가지고 있는지 확인해야 합니다.

#### Solution

유니코드에서 어떤 캐릭터는 하나 이상의 유효한 코드 포인트의 `sequence`로 표현될 수 있습니다.

```python
s1 = 'Spicy Jalape\u00f1o'
s2 = 'Spicy Jalapen\u0303o'
print(s1)
print(s2)
print(s1 == s2)    # False
print(len(s1))     # 14
print(len(s2))     # 15
```

텍스트는 두가지 형태로 표현됩니다. 처음은 전체적으로 구성된 캐릭터를 사용하고(U+00F1) 두번째는 라틴 문자 "n" 뒤에 "~" 캐릭터를 혼합하여 사용됩니다(U+0303).

여러 표현을 갖는 것은 프로그램의 문자열을 비교할 때 문제가 됩니다. 이를 해결하기 위해 `unicodedata` 모듈을 사용함으로써 표준 표현으로 텍스트를 먼저 정규화 할 수 있습니다.

```python
import unicodedata
t1 = unicodedata.normalize('NFC', s1)
t2 = unicodedata.normalize('NFC', s2)
print(t1 == t2)    # True
print(ascii(t1))   # 'Spicy Jalape\xf1o'
t3 = unicodedata.normalize('NFD', s1)
t4 = unicodedata.normalize('NFD', s2)
print(t3 == t4)    # True
print(ascii(t3))   # 'Spicy Jalapen\u0303o'
```

`normalize()`의 첫번째 인수는 어떻게 문자열을 정규화하기를 원하는지 지정합니다. `NFC`는 문자가 완전히 구성됨을 의미합니다.(즉, 가능한 경우 `single code point`를 사용합니다.) `NFD`는 문자 결합을 사용하여 문자가 완전히 분해되어야 함을 의미합니다.

Python은 `NFKC`와 `NFKD` 형태의 정규화도 지원하는데 이는 특정 종류의 문자를 처리하기 위한 추가적인 호환성 기능을 추가합니다.

```python
s = '\ufb01'    # A single character
unicodedata.normalize('NFD', s)

# Notice how the combined letters are broken apart here
unicodedata.normalize('NFKD', s)
unicodedata.normalize('NFKC', s)
```

#### Discussion

정규화는 정상적이고 일관성있는 방법에서 유니코드 텍스트를 처리를 보장해야하는 모든 코드에서 중요한 부분입니다. 이는 인코딩을 거의 제어할 수 없는 사용자 입력의 일부로 부터 받은 문자열을 처리할 때 특히 그렇습니다.

정규화는 또한 텍스트를 `filtering`하고 `sanitizing`하는 중요한 부분이 될 수 있습니다. 예를 들어 어떤 텍스트에서 모든 `discritical(분음)`부호를 제거하려면(매칭이나 검색 목적을 위해)

```python
t1 = unicodedata.normalize('NFD', s1)
print(''.join(c for c in t1 if not unicodedata.combining(c)))    # 'Spicy Jalapeno'
```

마지막 예시는 `unicodedata` 모듈의 또 다른 중요한 측면, 즉 문자를 다른 문자 클래스에 테스트하는 유틸리티 함수를 보여줍니다. `combining()` 함수는 문자가 결합문자인지 보기 위해 테스트합니다. 문자 카테고리를 찾거나 숫자를 테스트 하는 등을 위한 또 다른 함수가 모듈에 있습니다.

유니코드는 명백히 거대한 토픽입니다. 정규화에 대해서 좀더 자세한 정보를 찾으려면 [유니코드를 주제로 하는](//www.unicode.org/faq/normalization.html) 페이지를 방문하시길 바랍니다. Ned Batchelder의 [홈페이지](//nedbatchelder.com/text/unipain.html)에 Python 유니코드 처리에 대한 완벽한 프레젠테이션이 있습니다.

## 2.10 Regular Expression에서 유니코드를 사용하여 작업

#### Problem

텍스트를 처리하기 위해 `regular expression`을 사용하지만, 유니코드 문자의 `handling`에 대해 우려가 됩니다.

#### Solution

기본적으로 `re`모듈은 이미 특정 유니코드 문자 클래스에 대해 기본적으로 프로그래밍 되어 있습니다. 예를 들어 `\d`는 이미 모든 숫자 유니코드와 매치됩니다.

```python
import re
num = re.compile('\d+')
# ASCII disits
num.match('123')

# Arabic digits
num.match('\u0661\u0662\u0663')
```

지정된 유니코드 문자를 패턴에 포함해야 한다면, 유니코드 문자를 위한 일반적인 `escape sequence`(`\uFFFF` 또는 `\UFFFFFFF`)를 사용할 수 있습니다. 예를 들면 다음은 아주 약간 다른 Arabic 코드 페이지의 모든 문자에 매치되는 `regex`입니다.

```python
arabic = re.compile('[\u0600-\u06ff\u0750-\u077f\u08a0-\u08ff]+')
```

`matching`이나 `searching operation`을 수행할 때 정규화와 가능한 모든 텍스트를 일반 형태로 먼저 `sanitize`하는 것은 좋은 방법입니다. ([2.9장]() 참조) 하지만 특별한 케이스에 주의하는 것 역시 중요합니다. 예를 들어 대소문자를 구별하는 매칭에 `case folding`을 결합하는 것을 고려하면

```python
pat = re.compile('stra\u00dfe', re.IGNORECASE)
s = 'straße'
pat.match(s)          # Matches
pat.match(s.upper())  # Doesn't match
s.upper()             # Case folds
# 'STRASSE'
```

#### Discussion

유니코드와 `regular expression`을 섞는 것은 종종 머리 아프게 만들기 좋은 방법입니다. 심각하게 만들지 않으려면, 유니코드 `case folding`을 모두 제공하고 적절한 매칭을 포함해서 다른 흥미로운 기능도 가능한 다양하게 제공하는 서드 파티 `regex library` 설치를 고려해 봐야 합니다. 

## 2.11 문자열에서 원하지 않는 문자 Stripping

#### Problem

텍스트 문자열의 처음, 끝, 중간으로부터 `whitespace`같은 원하지 않는 문자를 `strip` 하려고 합니다.

#### Solution

`Strip()` 메서드는 문자를 문자열의 시작과 끝으로부터 `strip`하기 위해 사용할 수 있습니다. `lstrip()`과 `rstrip()`은 각자 왼쪽이나 오른쪽으로부터의 `strip`을 수행합니다. 기본적으로, 이런 메서드들은 `whitespace`를 `strip`하지만, 다른 문자가 주어질 수도 있습니다.

```python
# Whitespace stripping
s = '    hello world  \n'
print(s.strip())     # 'hello world'
print(s.lstrip())    # 'hello world  \n'
print(s.rstrip())    # '    hello world'

# Character stripping
t = '-----hello====='
print(t.lstrip('-')) # 'hello====='
print(t.strip('-=')) # 'hello'
```

#### Discussion

다양한 `strip()` 메서드가 나중에 읽기 및 데이터 정리할 때 일반적으로 사용됩니다. 예를 들어 `whitespace`나 `quotation` 또는 다른 작업을 제거하기 위해 이를 사용할 수 있습니다.

`stripping`이 문자열 중간의 모든 텍스트에 적용되지 않는 다는 점에 주의하셔야 합니다.

```python
s = '  hello    world    \n'
s = s.strip()
print(s)    # 'hello    world'
```

안쪽에 어떤 작업을 해야 한다면, `replace()` 메서드나 `regular expression` 대체를 사용하는 것 같이 다른 테크닉 사용이 필요할 수 있습니다.

```python
print(s.replace(' ', ''))    # 'helloworld'
import re
re.sub('\s+', ' ', s)        # 'hello world'
```

이는 종종 파일에서 데이터 라인을 읽는 것 같이 문자열 `stripping operation`과 다른 종류의 `iterative` 처리를 결합하길 원하는 경우가 됩니다. 그래서 `generator` 표현식이 유용할 수 있게 됩니다.

```python
with open(filename) as f:
    lines = (line.strip() for line in f)
    for line in lines:
        ...
```

여기서, 표현식 `lines = (line.strip() for line in f)` 은 데이터 변환을 진행합니다. 이는 실제로 먼저 어떤 종류의 일시적인 리스트 안의 데이터를 읽지 않기에 효율적입니다. 단지 모든 생성된 라인에 `stripping` 작업이 적용 되면서 `iterator`를 생성하게 됩니다.

좀 더 심화된 `stripping`을 위해 다음 단원의 `translate()` 메소드를 통해 문자열을 `sanitizing` 시킬 수 있을 것입니다.

## 2.12 Sanitizing and Cleaning Up Text

#### Problem

어떤 지루한 스크립트 어린이가 내 웹페이지에 “pýtĥöñ” 이라고 입력했고 이것을 어떻게든 정리하고 싶습니다.

#### Solution

`sanitizing`과 텍스트 정리의 문제는 `text parsing`과 `data handling`을 포함하는 넓은 범위의 다양한 문제에도 적용됩니다. 아주 간단한 레벨에서, 텍스트를 표준 케이스로 변환하기 위해 `str.upper()`와 `str.lower()` 같은 기본적인 문자열 함수를 사용할 수 있습니다. `str.replace()` 또는 `re.sub()`을 사용한 간단한 대체 작업으로 매우 특정한 문자 `sequence`를 지우거나 바꾸는데 집중할 수 있습니다. 또한 [2.9장]()에서와 같이 `unicodedata.normalize()`를 사용하여 정규화를 시킬 수도 있습니다.

하지만 한단계 더 나간 `sanitation process` 를 취하려고 합니다. 예를 들면, 아마 `diacritical`(분음) 기호를 `strip`하거나 문자의 전체적인 범위를 제거하고 싶을 것입니다. 그렇게 하려면 `str.translate()` 메서드를 통해 바꿀 수 있습니다. 만약 다음과 같이 지저분한 문자열이 있다면

```python
s = 'pýtĥöñ\fis\tawesome\r\n'
print(s)    # 'pýtĥöñ\x0cis\tawesome\r\n'
```

첫번째 과정은 `whitespace`를 정리하는 것입니다. 이로써 작은 `translation table`이 만들어지고 `translate()`를 사용합니다.

```python
remap = {
    ord('\t') : ' ',
    ord('\f') : ' ',
    ord('\r') : None     # Deleted
}
a = s.translate(remap)
print(a)        # 'pýtĥöñ is awesome'
```

위에서 보듯이 `\t`와 `\f` 같은 `whitespace` 문자가 하나의 `space`로 `remap`되었습니다. 캐리지 리턴을 의미하는 `\r`도 완전히 제거되었습니다.

이런 `remapping` 아이디어를 적용하여 한단계 나갈 수 있고, 좀 더 큰 테이블도 만들 수 있습니다. 예를 들어 모든 결합 문자를 제거해봅시다.

```python
import unicodedata
import sys
cmb_chrs = dict.fromkeys(c for c in range(sys.maxunicode) if unicodedata.combining(chr(c)))

b = unicodedata.normalize('NFD', a)
print(b)                       # 'pýtĥöñ is awesome\n'
print(b.translate(cmb_chrs))   # 'python is awesome\n'
```

마지막 예제에서 모든 유니코드 결합문자를 `None`에 `dictionary mapping`하는 것은 `dict.fromkeys()`를 사용하여 만들어진 것입니다.

원본 `input`은 `unicodedata.normalize()`를 사용하여 분리된 형태로 정규화 됩니다. 거기서 부터 `translate` 함수가 모든 악센트를 제거하는 데에 사용됩니다. 다른 문자 종류(`control character` 등)를 제거하기 위해서도 비슷한 테크닉이 사용될 수 있습니다.

또다른 예제로 모든 유니코드 숫자 캐릭터를 동등한 ASCII에 매핑한 `translation table`도 있습니다.

```python
digitmap = {c: ord('0') + unicodedata.digit(chr(c)) for c in range(sys.maxunicode) if unicodedata.category(chr(c)) == 'Nd'}
print(len(digitmap))          # 460

# Arabic digits
x = '\u0661\u0662\u0663'
print(x.translate(digitmap))  # '123'
```

아직 I/O 디코딩과 인코딩 함수를 포함하는 텍스트를 정리하기 위한 다른 테크닉이 있습니다. 여기서의 아이디어는 처음 예비로 텍스트를 정리하고,  `encode()`와 `decode()` 함수의 결합을 통해 텍스트를 `strip`하고 변경하는 것입니다.

```python
b = unicodedata.normalize('NFD', a)
print(b.encode('ascii', 'ignore').decode('ascii'))
# 'python is awesome\n'
```

이 정규화가 문자와 함께 나뉘어진 결합 문자에서 원본 텍스트의 분해된 문자를 처리합니다. 후속 ASCII 인코딩/디코딩으로 이런 모든 문자들이 단순히 한꺼번에 버려집니다. 당연히 ASCII 표현을 최종목표로 얻은 작업일 경우에만 작동할 것입니다.

#### Discussion

텍스트 `sanitizing`의 주된 이슈는 `runtime performance`가 됩니다. 일반적인 규칙에 따라 더 간단할수록, 더 빠르게 실행될 것입니다. 간단한 대체작업으로, `str.replace()` 메서드는 심지어 여러 번 호출하더라도 가장 빠른 접근이 됩니다. 

```python
def clean_spaces(s):
    s = s.replace('\r', '')
    s = s.replace('\t', ' ')
    s = s.replace('\f', ' ')
    return s
```

이를 시도한다면, `translate()`나 `regular expression`을 사용하여 접근하는 것 보다 훨씬 빠르게 찾을 수 있습니다.

다른 방면으로, `translate()` 메서드는 여러 일반적이지 않은 문자를 `remapping`하거나 제거를 수행해야 할 때 매우 빠릅니다.

큰 그림에서, 성능은 특정 애플리케이션에서 더 많은 연구를 해야합니다. 불행히도 모든 케이스에 가장 적합한 특정 테크닉 하나를 제안하기는 불가능합니다. 그러므로 다양한 접근으로 측정해봐야 합니다.

이런 해결책이 텍스트에 집중되더라도 비슷한 테크닉이 간단한 `replacements`, `translation`, `regular expression`을 포함한 `byte`에서도 적용될 수 있습니다.

## 2.13 Aligning Text Strings

#### Problem

텍스트 형태를 `alignment`(좌우 정렬)가 적용된 어떤 `sort`(순서 정렬)를 가진 텍스트 포맷이 필요합니다.

#### Solution

문자열 `alignment`의 기본을 위해 `ljust()`, `rjust()`, `center()` 문자열 메서드를 사용할 수 있습니다.

```python
text =  'Hello World'
print(text.ljust(20))    # 'Hello World         '
print(text.rjust(20))    # '         Hello World'
print(text.center(20))   # '    Hello World     '
```

이 모든 메서드들은 부가적인 `&&65.180&&`문자들로 채우는 것도 허용됩니다.

```python
print(text.rjust(20, '='))  # '=========Hello World'
print(text.center(20, '*')  # '****Hello World*****'
```

`format()` 함수는 또한 `align`을 쉽게 사용할 수 있습니다. 하고자 하는 모든 것을 `<`, `>`, 또는 `^` 문자와 함께 원하는 너비로 사용할 수 있습니다.

```python
format(text, '>20')    # '         Hello World'
format(text, '<20')    # 'Hello World         '
format(text, '^20')    # '    Hello World     '
```

`space` 보다 문자를 포함하고 싶다면 `alignment character` 앞에 지정하면 됩니다.

```python
format(text, '=>20s')  # '=========Hello World'
format(text, '*^20s')  # '****Hello World*****'
```

이런 형태의 코드는 `format()` 메서드를 여러 변수의 형태로 표현할 때 사용될 수 있습니다.

```python
'{:>10s} {:>10s}'.format('Hello', 'World')
# '     Hello     World'
```
`format()`의 하나의 이점은 문자열을 지정하지 않는다는 것입니다. 어떤 변수와 함께라도 작동하고, 더 일반적인 목적으로 만들 수 있습니다.

```python
x = 1.2345
format(x, '>10')    # '    1.2345'
format(x, '^10.2f)  # '   1.23   '
```

#### Discussion

예전 코드에서는 `% operator`도 `format text`에 사용된 것을 봤을 것입니다.

```python
`%-20s' % text      # 'Hello World         '
'%20s' % text       # '         Hello World'
```

하지만 새로운 코드에서는 `format()` 함수나 메서드를 사용하는 것이 선호되어야 합니다. `format()`이 `% operator`로 제공된 것 보다 훨씬 파워풀합니다. 게다가 `format()`은 좀 더 일반적으로 `ljust()`, 'rjust()` 또는 `center()` 문자열 메서드를 어떤 종류의 객체와 함께라도 사용할 수 있습니다.

좀더 완전한 `format()` 함수의 사용 가능한 기능의 목록을 위해, [Python 온라인 문서]()를 참고하시기 바랍니다.