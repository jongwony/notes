---
layout: post
title: Python cookbook Chapter 2 한글
tags: ['python', 'docs', 'string', 'parsing', 'tokenize', '한글']
progress: 99
---

<div class='warn'>
이 문서는 Python Coobook 3rd edition - O'REILLY, David Beazley & Brian K. jones 를 참고한 것이며 <a href="//wikidocs.net/book/1">Python을 완전히 처음 접하는 경우</a>에는 적합하지 않습니다.<br>개인적으로 공부한 내용이라 오역이 있을 수 있으며 <strong>Memo</strong>에는 실제 C를 이용한 테스트를 하지 않았으므로 이해를 위한 추측성 부분이 많음에 유의하시기 바랍니다.<br>
</div>

거의 모든 유용한 프로그램은 데이터를 파싱하던지 또는 출력을 생성하는 일종의 텍스트 처리를 포함합니다. 이 챕터는 문자열 분리, searching, substitution, lexing, parsing과 같은 텍스트 조작을 포함하는 일반적인 문제에 초점을 맞춥니다. 이러한 많은 작업은 `string`의 `built-in method`를 사용하여 쉽게 해결할 수 있습니다. 하지만 좀더 복잡한 연산을 수행하려면 `정규 표현식s`를 사용하거나 본격적인 `parser`를 만들어야 할 수 있습니다. 이 모든 주제들을 담았습니다. 또한 유니코드로 작업할 때 몇가지 까다로운 부분을 다룹니다.

## 2.1 복합적인 Delimiter에서 문자열 분리

#### Problem

필드에서 문자열 분리를 하고 싶은데 `delimiter(구분 기호)` (및 그 주위의 공백)가 문자열 전체에서 일관성이 없습니다.

#### Solution

문자열 객체에서의 `split()` 메서드는 매우 단순한 경우를 의미하고, 복합적인 `delimiter`를 허용하지 않거나 `delimiter` 주위의 가능한 `whitespace`를 고려하지 못합니다. 좀더 유연할 필요가 있을 때는 `re.split()` 메서드를 사용합니다.

```python
>>> line = 'asdf fjdk; afed, fjek,asdf,         foo'
>>> import re
>>> re.split(r'[;,\s]\s*', line)
['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']
```

#### Discussion

`re.split()` 함수는 분리 기호에 복합적인 패턴을 지정할 수 있기 때문에 유용합니다. 예를 들어 위의 solution에서 보듯이 분리 기호는 `,` `;` 또는 `whitespace(\s)` 뒤의 임의의 추가적인 `whitespace` 입니다. 패턴이 발견될 때마다, 전체적으로 매치되는 부분은 매치된 양 쪽에 있는 필드 사이에서 `delimiter`가 됩니다. 결과는 `str.split()`과 같이 필드들의 `list`를 반환합니다.

`re.split()`을 사용할 때는 괄호 안에 `capture group`을 포함하는 정규 표현식 패턴을 약간 조심해야 할 필요가 있습니다. 그 `capture group` 사용한다면 매치된 텍스트도 결과에 포함됩니다.

```python
>>> fields = re.split(r'(;|,|\s)\s*', line)
>>> fields
['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo']
```

분리된 `character`를 얻는 것은 특정 상황에 유용할 수 있습니다. 예를 들어 나중에 출력 문자열을 수정하려면 `split character`가 필요할 수 있습니다.

```python
>>> values = fields[::2]
>>> delimiters = fields[1::2] + ['']
>>> values
['asdf, 'fjdk', 'afed', 'fjek', 'asdf', 'foo']
>>> delimiters
[' ', ';', ',', ',', ',', '']
>>> # Reform the line using the same delimiters
>>> ''.join(v+d for v, d in zip(values, delimiters))
'asdf fjdk;afed,fjek,asdf,foo'
```

결과의 구분 기호를 사용하고 싶지 않지만 여전히 괄호를 사용하여 정규 표현식 패턴의 그룹을 사용하고자 한다면 `(?:...)`로 지정되는 `noncapture group`을 사용하면 됩니다.

```python
>>> re.split(r'(?:,|;|\s)\s*', line)
['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']
```

#### Memo

문자열 앞의 `r`은 그 문자열이 `raw string`이라는 것을 알려주는 것입니다.

백슬래시 충돌 문제를 이 `r`을 통해 해결할 수 있습니다.

[참고](//wikidocs.net/4308)

## 2.2 문자열의 처음과 끝에 매칭되는 텍스트

#### Problem

파일 이름이나 확장자, `URL scheme` 등 특정 텍스트 패턴의 문자열의 시작 또는 끝을 체크할 필요가 있습니다.

#### Solution

문자열의 시작 또는 끝을 체크하는 간단한 방법은 `str.startswith()` 또는 `str.endswith()` 메서드를 사용하는 것입니다.

```python
>>> filename = 'spam.txt'
>>> filename.endswith('.txt')
True
>>> filename.startswith('file:')
False
>>> url = 'http://www.python.org'
>>> url.startswith('http:')
True
```

여러 선택에 대해 체크해야 하는 경우 간단히 `startswith()` 또는 `endswith()`에 가능한 `tuple`을 제공하면 됩니다.

```python
>>> import os
>>> filenames = os.listdir('.')
>>> filenames
['Makefile', 'foo.c', 'bar.py', 'spam.c', 'spam.h']
>>> [name for name in filenames if name.endswith(('.c', '.h'))]
['foo.c', 'spam.c', 'spam.h']
>>> any(name.endswith('.py') for name in filenames))
True
```

여기 다른 예시도 있습니다.

```python
from urllib.request import urlopen

def read_data(name):
    if name.startswith(('http:', 'https:', 'ftp:')):
        return urlopen(name).read()
    else:
        with open(name) as f:
            return f.read()
```

이상하게도 Python의 한 부분에서 `tuple`이 실제로 `input`으로 요구됩니다. `list`나 `set`으로 지정된 선택을 했다면 `tuple`을 먼저 사용함으로써 변환해 주어야 합니다.

```python
>>> choices = ['http:', 'ftp:']
>>> url = 'http://www.python.org'
>>> url.startswith(choices)
TypeError: startswith first arg must be str or a tuple of str, not list
>>> url.startswith(tuple(choices))
True
```

#### Discussion

`startswith()`와 `endswith()` 메서드는 기본적인 `prefix`와 `suffix` 검사를 수행하기 위한 매우 편리한 방법을 제공합니다. `slice`를 써서 비슷한 작업이 수행될 수 있지만, 조금 아름다운 코드는 되지 못합니다.

```python
>>> filename = 'spam.txt'
>>> filename[-4:] == '.txt'
True
>>> url = 'http://www.python.org'
>>> url[:5] == 'http:' or url[:6] == 'https:' or url[:4] == 'ftp:'
True
```

또한 정규 표현식로 사용하려는 경향이 있을 경우가 있습니다.

```python
>>> import re
>>> url = 'http://python.org'
>>> re.match('http:|https:|ftp:', url)
```

이 역시 동작하지만, 종종 단순한 매치에는 과도합니다. Solution을 사용하는 것이 좀 더 간단하고 빠르게 실행됩니다.

마지막으로 하지만 적어도, `startswith()`와 `endswith()` 메서드가 다른 일반적인 `data reduction`같은 연산과 결합 될 때 보기 좋습니다. 예를 들어 특정 종류의 파일이 있는지 디렉터리를 체크하는 `if`문은 다음과 같습니다.

```python
if any(name.endswith(('.c', '.h')) for name in listdir(dirname)):
    pass
```

## 2.3 Shell Wildcard 패턴을 사용하여 문자열 매칭

#### Problem

`Unix shell`에서 동작할 때 일반적으로 사용되는 `wildcard`(`*.py,Dat[0-9]*.csv`와 같은) 패턴과 동일한 사용으로 텍스트를 매칭하고 싶습니다.

#### Solution

`fnmatch` 모듈은 어떤 매칭을 수행하는데 사용되는 두 함수 `fnmatch()`와 `fnmatchcase()`를 제공합니다. 사용은 간단합니다.

```python
from fnmatch import fnmatch, fnmatchcase
fnmatch('foo.txt', '*.txt')         # True
fnmatch('foo.txt', '?oo.txt')       # True
fnmatch('Dat45.csv', 'Dat[0-9]*')   # True

names = ['Dat1.csv', 'Dat2.csv', 'config.ini', 'foo.py']
print([name for name in names if fnmatch(name, 'Dat*.csv')])
# ['Dat1.csv', 'Dat2.csv']
```

일반적으로 `fnmatch()`는 파일 시스템 아래의(다양한 운영체제에 따라 다른) 동일한 대소문자를 구분하는데 사용하여 패턴을 매치합니다.

```python
# On OS X (Mac)
fnmatch('foo.txt', '*.TXT')         # False

# On Windows
fnmatch('foo.txt', '*.TXT')         # True
```

이런 구별이 중요해질 경우에는 대신 `fnmatchcase()`를 사용하시면 됩니다. 이는 제공한 대 소문자 규칙을 기반으로 정확하게 매치됩니다.

```python
from fnmatch import fnmatchcase
fnmatchcase('foo.txt', '*.TXT')     # False
```

이런 함수의 `overlooked feature`(포괄적인 기능)은 파일 이름이 아닌 문자열의 데이터 처리와 함께 잠재적으로 사용되는 것입니다.
예를 들어 다음과 같이 주소의 `list`를 가지고 있다고 하면

```python
addresses = [
    '5412 N CLARK ST',
    '1060 W ADDISON ST',
    '1039 W GRANVILLE AVE',
    '2122 N CLARK ST',
    '4802 N BROADWAY'
]
```

`list comprehension`을 다음과 같이 쓸 수 있습니다.

```python
from fnmatch import fnmatchcase
print([addr for addr in addresses if fnmatchcase(addr, * ST')])
# ['5412 N CLARK ST', '1060 W ADDISON ST', '2122 N CLARK ST']
print([addr for addr in addresses if fnmatchcase(addr, '54[0-9][0-9] *CLARK*')])
# ['5412 N CLARK ST']
```

#### Discussion

`fnmatch`로 수행된 매칭은 간단한 문자열 메서드의 기능과 정규 표현식의 모든 기능 사이의 어디에서나 위치합니다. 단지 데이터 처리 연산의 `wildcard`를 허용하기 위한 간단한 메커니즘을 제공하려면 종종 합리적인 해결책이 될 수 있습니다.

만약 실제 파일 이름을 매치하는 코드를 쓰려고 한다면 [5.13장]()의 `glob` 모듈을 대싱 사용하시면 됩니다.

## 2.4 텍스트 패턴을 위한 Matching과 Searching

#### Problem

특정 패턴을 위한 텍스트를 매치하거나 찾고 싶습니다.

#### Solution

텍스트를 간단히 글자 그대로 매치하려고 한다면, 단지 `str.find()`, `str.endswith()`, `str.startswith()`같은 기본적인 `string` 메서드를 사용할 수 있습니다.

```python
>>> text = 'yeah, but no, but yeah, but no, but yeah'
>>> # Exact match
>>> text == 'yeah'
False
>>> # Match at start or end
>>> text.startswith('yeah')
True
>>> text.endswith('no')
False
>>> # Search for the location of the first occurrence
>>> text.find('no')
10
```

좀 더 복잡한 매칭을 위해서는 `re` 모듈과 정규 표현식을 사용하면 됩니다. 정규 표현식의 기본적인 메커니즘을 설명하기 위해 `11/27/2012` 같이 날짜를 지정된 숫자에 매치하길 원한다고 하면

```python
text1 = '11/27/2012'
text2 = 'Nov 27, 2012'

import re
# Simple matching: \d+ means match one or more digits
def textmatch(text)
    if re.match(r'\d+/\d+/\d+', text):
        print('yes')
    else:
        print('no')

textmatch(text1)        # yes
textmatch(text2)        # no
```

같은 패턴을 사용하여 많은 양의 매칭을 수행하려고 하면 정규 표현식 패턴을 먼저 패턴 객체로 `precompile`하는 것이 일반적인 방법입니다.

```python
datepat = re.compile(r'\d+/\d+/\d+')

def textmatch(text):
    if datepat.match(text):
        print('yes')
    else:
        print('no')

textmatch(text1)        # yes
textmatch(text2)        # no
```

`match()`는 항상 문자열의 시작부분에서 일치하는 부분을 찾으려고 합니다. 패턴이 일어나는 모든 부분의 텍스트를 찾으려면 `findall()` 메서드를 대신 사용하면 됩니다.

```python
text = 'Todays is 11/27/2012. PyCon starts 3/13/2013.'
print(datepat.findall(text)) # ['11/27/2012', '3/13/2013']
```

정규 표현식을 정의할 때는 괄호 안의 패턴의 일부분을 묶어 `capture group`을 도입하는 것이 일반적입니다.

```python
datepat = re.compile(r'(\d+)/(\d+)/(\d+)')
```

`capture group`들은 각 그룹의 내용을 개별적으로 추출할 수 있기 때문에 매치된 텍스트의 후속 처리를 단순화하는 것입니다.

```python
m = datapat.match('11/27/2012')

# Extract the contents of each group
print(m.group(0))       # '11/27/2012'
print(m.group(1))       # 11
print(m.group(2))       # 27
print(m.group(3))       # 2012
print(m.groups())       # ('11', '27', '2012')

# Find all matches (notice splitting into tuples)
text = 'Today is 11/27/2012. PyCon starts 3/13/2013'
datepat.findall(text)   # [('11', '27', '2012'), ('3', '13', '2013')]

for month, day, year in datepat.findall(text):
    print('{}-{}-{}'.format(year, month, day))
```

`findall()` 메서드는 텍스트를 검색하고 모든 매치를 찾아 그 리스트를 반환합니다. 매치를 순회하면서 찾으려면 `finditer()` 메서드를 사용하시면 됩니다.

```python
for m in datepat.finditer(text):
    print(m.groups())
```

#### Discussion

정규 표현식의 이론에 대한 기본적인 튜토리얼은 이 책의 범위를 벗어납니다. 하지만 위에서 소개한 방법은 `re` 모듈을 사용하여 텍스트를 검색하고 매칭하는 데 절대적인 기본 사항을 설명합니다. 필수적인 기능은 `re.compile()`을 사용하여 처음 패턴을 컴파일 하고 `match()`, `findall()`, 또는 `finditer()`같은 메서드를 사용하는 것입니다.

패턴을 지정할 때는 `'r(\d+)/(\d+)/(\d+)'`와 같은 `raw string`을 사용하는 것이 상대적으로 일반적입니다. 이런 문자열은 백슬래시 문자를 해석되지 않고 남기며 정규 표현식의 내용에 유용할 수 있습니다. 그렇지 않으면 이중 백슬래시를 `'(\\d+)/(\\d+)/(\\d+)'`와 같이 사용해야 합니다.

`match()` 메서드가 문자열의 시작 부분 부터만 체크한다는 것에 주의하셔야 합니다. 원하지 않는 매칭이 일어날 수 있습니다.

```python
# 앞 부분 부터 맞기만 하면 됨 (정확한 매칭은 아님)
m = datepat.match('11/27/2012abcdef')
print(m.group())        # '11/27/2012'
```

정확한 매칭을 하려면 패턴에 `end-marker($)`를 포함해야 합니다.

```python
datepat = re.compile(r'(\d+)/(\d+)/(\d+)$')
print(datepat.match('11/27/2012abcdef'))        # None
print(datepat.match('11/27/2012'))              # <_sre.SRE_Match object; span=(0, 10), match='11/27/2012'>
```

마지막으로 단순하게 텍스트 매칭/검색 만 수행하려면 컴파일 단계를 건너 뛰고 `re` 모듈의 모듈 레벨 함수를 대신 사용하는 방법이 있습니다.

```python
re.findall(r'(\d+)/(\d+)/(\d+)', text)           # [('11', '27', '2012'), ('3', '13', '2013')]
```

그러나 매칭이나 검색을 많이 수행할 경우에는, 보통 패턴을 먼저 컴파일 하고 계속해서 사용하는 것이 좋습니다. 모듈 레벨의 함수는 최근에 컴파일 된 패턴의 캐시를 유지하므로, 성능에 크게 영향을 주진 않지만 지신이 직접 컴파일 한 패턴을 사용함으로써 추가적인 처리와 몇가지 조회를 줄일 수는 있습니다.


## 2.5 Searching and Replacing Text

#### Problem

문자열의 텍스트패턴을 검색하고 대체하고 싶습니다.

#### Solution

글자 그대로의 간단한 패턴에서는 `str.replace()`메서드를 사용하면 됩니다.

```python
text = 'yeah, but no, but yeah, but no, but yeah'
text.replace('yeah', 'yep')    # 'yep, but no, but yep, but no, but yep'
```

좀 더 복잡한 패턴에서는 `re` 모듈의 `sub()` 함수/메서드를 사용하면 됩니다. `11/27/2012`를 `2012-11-27`로 다시 쓰고 싶다면 간단하게 다음과 같이 하면 됩니다.

```python
text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'
import re
re.sub(r'(\d+)/(\d+)/(\d+)', r'\3-\1-\2', text)   # 'Today is 2012-11-27. PyCon starts 2013-3-13.'
```

`sub()`의 첫 번째 인수는 매치할 패턴이고 두 번째 인수는 대체할 패턴입니다. `\3`과 같이 백슬래시된 숫자는 패턴에서의 `capture group` 숫자를 가리킵니다.

만약 같은 패턴에 대한 반복된 대체를 수행하려면 먼저 컴파일을 고려하는 것이 좋은 성능을 낼 수 있습니다.

```python
import re
datepat = re.compile(r'(\d+)/(\d+)/(\d+)')
datepat.sub(r'\3-\1-\2', text)
```

좀 더 복잡한 대체를 하려면 대체 `callback` 함수를 지정할 수 있습니다.

```python
from calendar import month_abbr
def change_date(m):
    mon_name = month_abbr[int(m.group(1))]
    return '{} {} {}'.format(m.group(2), mon_name, m.group(3))

datepat.sub(change_date, text)    # 'Today is 27 Nov 2012. PyCon Starts 13 Mar 2013.'
```

입력에서, 대체 `callback`에 대한 인수는 `match()` 또는 `find()` 객체로 반환된 매치된 객체입니다. 매치에서 지정된 부분을 추출하기 위해서는 `group()` 메서드를 사용합니다. 그 함수는 바뀐 텍스트를 반환하게 됩니다.

대체된 텍스트를 얻는 것에 추가로 얼마나 많이 대체되었는지 알고 싶을 때는 `re.subn()`을 대신 사용합니다.

```python
newext, n = datepat.subn(r'\3-\1-\2', text)
print(newtext)    # 'Today is 2012-11-27. PyCon starts 2013-3-13.'
print(n)          # 2
```

#### Discussion

정규 표현식 검색 및 대체에 대해 위의 `sub()` 메서드 보다 많은 것은 없습니다. 가장 까다로운 부분은 정규 표현식 패턴을 지정하는 것입니다. 이 부분은 독자에게 연습 문제로 남기는 것이 가장 좋습니다.

#### Memo

정규 표현식에 대한 부분

- [perl 정규 표현식](//www.perl.or.kr/perl_iyagi/regexp)

- [정규 표현식 visualization](//regexper.com)

- [정규 표현식 book](//shop.oreilly.com/product/9780596528126.do)

## 2.6 Searching and Replacing Case-Insensitive Text

#### Problem

대소문자를 구분하지 않고 텍스트를 검색하고 대체할 필요가 있습니다.

#### Solution

대소문자를 구분하지 않는 텍스트 작업을 수행하려면 `re` 모듈을 사용하고 다양한 작업을 위해 `re.IGNORECASE` 플래그를 제공해야 합니다.

```python
text = 'UPPER PYTHON, lower python, Mixed Python'
print(re.findall('python', text, flags=re.IGNORECASE))        # ['PYTHON', 'python', 'Python']
print(re.sub('python', 'snake', text, flags=re.IGNORECASE))   # 'UPPER snake, lower snake, Mixed snake'
```

마지막 예시는 텍스트 대체가 매치된 텍스트로 대소문자 변환이 되지 않는다는 한계를 보여줍니다.
이를 고치고 싶으면 다음과 같은 지원 함수를 사용해야 할 수 있습니다.

```python
def matchcase(word):
    def replace(m):
        text = m.group()
        if text.isupper():
            return word.upper()
        elif text.islower():
            return word.lower()
        elif text[0].isupper():
            return word.capitalize()
        else:
            return word
    return replace

re.sub('python', matchcase('snake'), text, flags=re.IGNORECASE)  # 'UPPER SNAKE, lower snake, Mixed Snake'
```

#### Discussion

간단한 경우에는 `re.IGNORECASE`를 제공함으로써 충분히 대소문자를 구분하지 않는 매칭을 수행할 수 있습니다. 하지만 대소문자 `folding`과 관련된 유니코드 매칭에는 충분하지 않을 수 있음에 주의하셔야 합니다. [2.10장]()에 자세한 내용이 있습니다.

## 2.7 가장 짧은 매치를 위한 정규 표현식 지정하기

#### Problem

'정규 표현식`을 사용하여 텍스트 패턴을 일치시키려 하지만 가장 긴 패턴만 식별합니다. 대신에 가장 짧은 매치를 찾게 바꾸려고 합니다.

#### Solution

인용 부호 같이 시작과 끝의 `delimiter` 쌍 안쪽 부분의 텍스트를 매칭하려 하는 패턴에서 이 문제가 종종 일어납니다.

```python
str_pat = re.compile(r'\"(.*)\"')
text1 = 'Computer says "no."'
print(str_pat.findall(text1))    # ['no.']
text2 = 'Computer says "no.", Phone says "yes."'
print(str_pat.findall(text2))    # ['no." Phone says "yes.']
```

이 예제에서는 `r'\"(.*)\"'` 패턴이 인용 부호로 묶인 텍스트를 찾으려고 합니다. 하지만 정규 표현식의 `* operator`가 욕심이 많아 매칭은 가장 긴 매치를 찾습니다. 그러므로 `text2`를 포함하는 두번째 예제에서는 두 인용부호 문자열에서 매치가 부정확합니다.

이를 고치려면 패턴에서 `? modifier`를 `* operator` 뒤에 추가합니다.

```python
str_pat = re.compile(r'\"(.*?)\"')
print(str_pat.findall(text2))    # ['no.', 'yes.']
```

이는 매칭의 욕심을 없애고 대신 가장 짧은 매치로 만들어 줍니다.

#### Discussion

이 방법은 `.` 문자의 사용을 포함하는 정규 표현식 하나 이상의 일반적인 문제를 다룹니다. 패턴에서 점은 개행(newline)을 제외한 모든 문자를 매치합니다. 반면에 인용 부호 같이 시작과 끝 텍스트로 점을 괄호로 묶는 경우에는 패턴 매치에 가능한 가장 긴 매치를 찾으려고 합니다. 이로 인해 시작 또는 끝 텍스트가 여러번 건너뛰고 가장 길게 매치된 결과를 포함하게 됩니다. `?`를 `*` 또는 `+` 연산의 오른쪽에 추가하는 것은 매칭 알고리즘이 가능한 가장 짧은 매치를 대신 찾게끔 합니다.

## 2.8 여러 라인의 패턴을 위한 정규 표현식 사용

#### Problem

정규 표현식을 사용하여 텍스트의 블록을 매치하려고 합니다. 하지만 여러줄에 걸친 매치가 필요합니다.

#### Solution

이 문제는 점을 사용하는 패턴에서 일반적으로 일어나지만 줄바꿈을 매치하지는 않는 사실을 잊지 말아야 합니다. 다음과 같이 C 스타일의 주석을 매치하려고 하면

```python
comment = re.compile(r'/\*(.*?)\*/')
text1 = '/* this is a comment */'
text2 = '''/* this is a
multiline comment */
'''

print(comment.findall(text1))    # [' this is a comment ']
print(comment.findall(text2))    # []
```

이 문제를 해결하려면 줄바꿈을 위한 지원을 추가할 수 있습니다.

```python
comment = re.compile(r'/\*((?:.|\n)\*/')
print(comment.findall(text2))    # [' this is a\n multiline comment ']
```

이 패턴에서, `(?:.|\n)`은 `noncapture group`을 지정합니다. (즉 이 그룹은 매칭할 그룹을 정의하지만, 해당 그룹은 개별적으로 캡쳐되거나 번호가 매겨지지는 않습니다.)

#### Discussion

`re.compile()` 함수는 여기서 유용한 `re.DOTALL` 플래그를 받습니다. 이는 정규 표현식의 `.`을 줄바꿈을 포함한 모든 캐릭터에 매치시킵니다.

```python
comment = re.compile(r'/\*(.*?)\*/', re.DOTALL)
print(comment.findall(text2))    # [' this is a\n multiline comment ']
```

`re.DOTALL` 플래그를 사용함으로써 간단한 케이스에 대해 잘 작동하지만 매우 복잡한 패턴 을 사용하거나 [2.18장]()에 소개된 것과 같이 토큰화의 목적을 위해 함께 사용하게 되는 별도의 정규 표현식을 혼합하여 사용하는 경우에는 문제가 될 수 있습니다.
선택이 주어진다면 여분의 플래그가 필요 없이 올바르게 작동하도록 정규 표현식 패턴을 정의하는 것이 좋습니다.

## 2.9 표준 표현을 위한 유니코드 텍스트 정규화

#### Problem

유니코드 문자열로 작업하지만 모든 문자열이 동일한 기본 표현을 가지고 있는지 확인해야 합니다.

#### Solution

유니코드에서 어떤 캐릭터는 하나 이상의 유효한 코드 포인트의 `sequence`로 표현될 수 있습니다.

```python
>>> s1 = 'Spicy Jalape\u00f1o'
>>> s2 = 'Spicy Jalapen\u0303o'
>>> s1
'Spicy Jalapeño'
>>> s2
'Spicy Jalapeño'
>>> s1 == s2
False
>>> len(s1)
14
>>> len(s2)
15
```

`Spicy Jalapeño` 텍스트는 두가지 형태로 표현됩니다. 처음은 전체적으로 구성된 `ñ` 문자를 사용하고(U+00F1) 두번째는 라틴 문자 `n` 뒤에 `~` 문자를 혼합하여 사용됩니다(U+0303).

여러 표현을 갖는 것은 프로그램의 문자열을 비교할 때 문제가 됩니다. 이를 해결하기 위해 `unicodedata` 모듈을 사용함으로써 표준 표현으로 텍스트를 먼저 정규화 할 수 있습니다.

```python
>>> import unicodedata
>>> t1 = unicodedata.normalize('NFC', s1)
>>> t2 = unicodedata.normalize('NFC', s2)
>>> t1 == t2
True
>>> print(ascii(t1))
'Spicy Jalape\xf1o'
>>> t3 = unicodedata.normalize('NFD', s1)
>>> t4 = unicodedata.normalize('NFD', s2)
>>> t3 == t4
True
>>> print(ascii(t3))
'Spicy Jalapen\u0303o'
```

`normalize()`의 첫번째 인수는 어떻게 문자열을 정규화하기를 원하는지 지정합니다. `NFC`는 문자가 완전히 구성됨을 의미합니다.(즉, 가능한 경우 `single code point`를 사용합니다.) `NFD`는 문자 결합을 사용하여 문자가 완전히 분해되어야 함을 의미합니다.

Python은 `NFKC`와 `NFKD` 형태의 정규화도 지원하는데 이는 특정 종류의 문자를 처리하기 위한 추가적인 호환성 기능을 추가합니다.

```python
>>> s = '\ufb01'    # A single character
>>> s
'ﬁ'
>>> unicodedata.normalize('NFD', s)
'ﬁ'
>>> # Notice how the combined letters are broken apart here
>>> unicodedata.normalize('NFKD', s)
'fi'
>>> unicodedata.normalize('NFKC', s)
'fi'
```

#### Discussion

정규화는 정상적이고 일관성있는 방법에서 유니코드 텍스트를 처리를 보장해야하는 모든 코드에서 중요한 부분입니다. 이는 인코딩을 거의 제어할 수 없는 사용자 입력의 일부로 부터 받은 문자열을 처리할 때 특히 그렇습니다.

정규화는 또한 텍스트를 `filtering`하고 `sanitizing`하는 중요한 부분이 될 수 있습니다. 예를 들어 어떤 텍스트에서 모든 `discritical(분음)`부호를 제거하려면(매칭이나 검색 목적을 위해)

```python
>>> t1 = unicodedata.normalize('NFD', s1)
>>> ''.join(c for c in t1 if not unicodedata.combining(c))
'Spicy Jalapeno'
```

마지막 예시는 `unicodedata` 모듈의 또 다른 중요한 측면, 즉 문자를 다른 문자 클래스에 테스트하는 유틸리티 함수를 보여줍니다. `combining()` 함수는 문자가 결합문자인지 보기 위해 테스트합니다. 문자 카테고리를 찾거나 숫자를 테스트 하는 등을 위한 또 다른 함수가 모듈에 있습니다.

유니코드는 명백히 거대한 토픽입니다. 정규화에 대해서 좀더 자세한 정보를 찾으려면 [유니코드를 주제로 하는](//www.unicode.org/faq/normalization.html) 페이지를 방문하시길 바랍니다. [Ned Batchelder 홈페이지](//nedbatchelder.com/text/unipain.html)에 Python 유니코드 처리에 대한 완벽한 프레젠테이션이 있습니다.

## 2.10 정규 표현식에서 유니코드를 사용하여 작업

#### Problem

텍스트를 처리하기 위해 정규 표현식을 사용하지만, 유니코드 문자의 `handling`에 대해 우려가 됩니다.

#### Solution

기본적으로 `re`모듈은 이미 특정 유니코드 문자 클래스에 대해 기본적으로 프로그래밍 되어 있습니다. 예를 들어 `\d`는 이미 모든 숫자 유니코드와 매치됩니다.

```python
import re
num = re.compile('\d+')
# ASCII disits
num.match('123')

# Arabic digits
num.match('\u0661\u0662\u0663')
```

지정된 유니코드 문자를 패턴에 포함해야 한다면, 유니코드 문자를 위한 일반적인 `escape sequence`(`\uFFFF` 또는 `\UFFFFFFF`)를 사용할 수 있습니다. 예를 들면 다음은 아주 약간 다른 Arabic 코드 페이지의 모든 문자에 매치되는 `regex`입니다.

```python
arabic = re.compile('[\u0600-\u06ff\u0750-\u077f\u08a0-\u08ff]+')
```

`matching`이나 `searching` 작업을 수행할 때 정규화와 가능한 모든 텍스트를 일반 형태로 먼저 `sanitize`하는 것은 좋은 방법입니다. ([2.9장]() 참조) 하지만 특별한 케이스에 주의하는 것 역시 중요합니다. 예를 들어 대소문자를 구별하는 매칭에 `case folding`을 결합하는 것을 고려하면

```python
>>> pat = re.compile('stra\u00dfe', re.IGNORECASE)
>>> s = 'straße'
>>> pat.match(s)          # Matches
>>> pat.match(s.upper())  # Doesn't match
>>> s.upper()             # Case folds
'STRASSE'
```

#### Discussion

유니코드와 정규 표현식을 섞는 것은 종종 머리 아프게 만들기 좋은 방법입니다. 심각하게 만들지 않으려면, 유니코드 `case folding`을 모두 제공하고 적절한 매칭을 포함해서 다른 흥미로운 기능도 가능한 다양하게 제공하는 서드 파티 `regex` 라이브러리 설치를 고려해 봐야 합니다.

## 2.11 문자열에서 원하지 않는 문자를 Stripping

#### Problem

텍스트 문자열의 처음, 끝, 중간으로부터 `whitespace`같은 원하지 않는 문자를 `strip` 하려고 합니다.

#### Solution

`strip()` 메서드는 문자를 문자열의 시작과 끝으로부터 `strip`하기 위해 사용할 수 있습니다. `lstrip()`과 `rstrip()`은 각자 왼쪽이나 오른쪽으로부터의 `strip`을 수행합니다. 기본적으로, 이런 메서드들은 `whitespace`를 `strip`하지만, 다른 문자가 주어질 수도 있습니다.

```python
# Whitespace stripping
s = '    hello world  \n'
print(s.strip())     # 'hello world'
print(s.lstrip())    # 'hello world  \n'
print(s.rstrip())    # '    hello world'

# Character stripping
t = '-----hello====='
print(t.lstrip('-')) # 'hello====='
print(t.strip('-=')) # 'hello'
```

#### Discussion

다양한 `strip()` 메서드가 나중에 읽기 및 데이터 정리할 때 일반적으로 사용됩니다. 예를 들어 `whitespace`나 `quotation` 또는 다른 작업을 제거하기 위해 이를 사용할 수 있습니다.

`stripping`이 문자열 중간의 모든 텍스트에 적용되지 않는 다는 점에 주의하셔야 합니다.

```python
s = '  hello    world    \n'
s = s.strip()
print(s)    # 'hello    world'
```

안쪽에 어떤 작업을 해야 한다면, `replace()` 메서드나 정규 표현식 대체를 사용하는 것 같이 다른 테크닉 사용이 필요할 수 있습니다.

```python
print(s.replace(' ', ''))    # 'helloworld'
import re
re.sub('\s+', ' ', s)        # 'hello world'
```

이는 종종 파일에서 데이터 라인을 읽는 것 같이 문자열 `stripping operation`과 다른 종류의 `iterative` 처리를 결합하길 원하는 경우가 됩니다. 그래서 `generator` 표현식이 유용할 수 있게 됩니다.

```python
with open(filename) as f:
    lines = (line.strip() for line in f)
    for line in lines:
        ...
```

여기서, 표현식 `lines = (line.strip() for line in f)` 은 데이터 변환을 진행합니다. 이는 실제로 먼저 어떤 종류의 일시적인 리스트 안의 데이터를 읽지 않기에 효율적입니다. 단지 모든 생성된 라인에 `stripping` 작업이 적용 되면서 `iterator`를 생성하게 됩니다.

좀 더 심화된 `stripping`을 위해 다음 단원의 `translate()` 메소드를 통해 문자열을 `sanitizing` 시킬 수 있을 것입니다.

## 2.12 Sanitizing and Cleaning Up Text

#### Problem

어떤 지루한 스크립트 어린이가 내 웹페이지에 `pýtĥöñ` 이라고 입력했고 이것을 어떻게든 정리하고 싶습니다.

#### Solution

`sanitizing`과 텍스트 정리의 문제는 `text parsing`과 `data handling`을 포함하는 넓은 범위의 다양한 문제에도 적용됩니다. 아주 간단한 레벨에서, 텍스트를 표준 케이스로 변환하기 위해 `str.upper()`와 `str.lower()` 같은 기본적인 문자열 함수를 사용할 수 있습니다. `str.replace()` 또는 `re.sub()`을 사용한 간단한 대체 작업으로 매우 특정한 문자 `sequence`를 지우거나 바꾸는데 집중할 수 있습니다. 또한 [2.9장]()에서와 같이 `unicodedata.normalize()`를 사용하여 정규화를 시킬 수도 있습니다.

하지만 한단계 더 나간 `sanitation process` 를 취하려고 합니다. 예를 들면, 아마 `diacritical`(분음) 기호를 `strip`하거나 문자의 전체적인 범위를 제거하고 싶을 것입니다. 그렇게 하려면 `str.translate()` 메서드를 통해 바꿀 수 있습니다. 만약 다음과 같이 지저분한 문자열이 있다면

```python
>>> s = 'pýtĥöñ\fis\tawesome\r\n'
>>> s
'pýtĥöñ\x0cis\tawesome\r\n'
```

첫번째 과정은 `whitespace`를 정리하는 것입니다. 이로써 작은 `translation table`이 만들어지고 `translate()`를 사용합니다.

```python
remap = {
    ord('\t') : ' ',
    ord('\f') : ' ',
    ord('\r') : None     # Deleted
}
a = s.translate(remap)
print(a)        # 'pýtĥöñ is awesome'
```

위에서 보듯이 `\t`와 `\f` 같은 `whitespace` 문자가 하나의 공백으로 `remap`되었습니다. 캐리지 리턴을 의미하는 `\r`도 완전히 제거되었습니다.

이런 `remapping` 아이디어를 적용하여 한단계 나갈 수 있고, 좀 더 큰 테이블도 만들 수 있습니다. 예를 들어 모든 결합 문자를 제거해봅시다.

```python
import unicodedata
import sys
cmb_chrs = dict.fromkeys(c for c in range(sys.maxunicode) if unicodedata.combining(chr(c)))

b = unicodedata.normalize('NFD', a)
print(b)                       # 'pýtĥöñ is awesome\n'
print(b.translate(cmb_chrs))   # 'python is awesome\n'
```

마지막 예제에서 모든 유니코드 결합문자를 `None`에 `dictionary mapping`하는 것은 `dict.fromkeys()`를 사용하여 만들어진 것입니다.

원본 `input`은 `unicodedata.normalize()`를 사용하여 분리된 형태로 정규화 됩니다. 거기서 부터 `translate` 함수가 모든 악센트를 제거하는 데에 사용됩니다. 다른 문자 종류(`control character` 등)를 제거하기 위해서도 비슷한 테크닉이 사용될 수 있습니다.

또다른 예제로 모든 유니코드 숫자 캐릭터를 동등한 ASCII에 매핑한 `translation table`도 있습니다.

```python
digitmap = {c: ord('0') + unicodedata.digit(chr(c)) for c in range(sys.maxunicode) if unicodedata.category(chr(c)) == 'Nd'}
print(len(digitmap))          # 460

# Arabic digits
x = '\u0661\u0662\u0663'
print(x.translate(digitmap))  # '123'
```

아직 I/O 디코딩과 인코딩 함수를 포함하는 텍스트를 정리하기 위한 다른 테크닉이 있습니다. 여기서의 아이디어는 처음 예비로 텍스트를 정리하고,  `encode()`와 `decode()` 함수의 결합을 통해 텍스트를 `strip`하고 변경하는 것입니다.

```python
b = unicodedata.normalize('NFD', a)
print(b.encode('ascii', 'ignore').decode('ascii'))
# 'python is awesome\n'
```

이 정규화가 문자와 함께 나뉘어진 결합 문자에서 원본 텍스트의 분해된 문자를 처리합니다. 후속 ASCII 인코딩/디코딩으로 이런 모든 문자들이 단순히 한꺼번에 버려집니다. 당연히 ASCII 표현을 최종목표로 얻은 작업일 경우에만 작동할 것입니다.

#### Discussion

텍스트 `sanitizing`의 주된 이슈는 `runtime performance`가 됩니다. 일반적인 규칙에 따라 더 간단할수록, 더 빠르게 실행될 것입니다. 간단한 대체작업으로, `str.replace()` 메서드는 심지어 여러 번 호출하더라도 가장 빠른 접근이 됩니다.

```python
def clean_spaces(s):
    s = s.replace('\r', '')
    s = s.replace('\t', ' ')
    s = s.replace('\f', ' ')
    return s
```

이를 시도한다면, `translate()`나 정규 표현식을 사용하여 접근하는 것 보다 훨씬 빠르게 찾을 수 있습니다.

다른 방면으로, `translate()` 메서드는 여러 일반적이지 않은 문자를 `remapping`하거나 제거를 수행해야 할 때 매우 빠릅니다.

큰 그림에서, 성능은 특정 애플리케이션에서 더 많은 연구를 해야합니다. 불행히도 모든 케이스에 가장 적합한 특정 테크닉 하나를 제안하기는 불가능합니다. 그러므로 다양한 접근으로 측정해봐야 합니다.

이런 해결책이 텍스트에 집중되더라도 비슷한 테크닉이 간단한 `replacements`, `translation`, 정규 표현식을 포함한 `byte`에서도 적용될 수 있습니다.

## 2.13 Aligning Text Strings

#### Problem

텍스트 형태를 `alignment`(좌우 정렬)가 적용된 어떤 `sort`(순서 정렬)를 가진 텍스트 포맷이 필요합니다.

#### Solution

문자열 `alignment`의 기본을 위해 `ljust()`, `rjust()`, `center()` 문자열 메서드를 사용할 수 있습니다.

```python
text =  'Hello World'
print(text.ljust(20))    # 'Hello World         '
print(text.rjust(20))    # '         Hello World'
print(text.center(20))   # '    Hello World     '
```

이 모든 메서드들은 부가적인 `&&65.180&&`문자들로 채우는 것도 허용됩니다.

```python
print(text.rjust(20, '='))  # '=========Hello World'
print(text.center(20, '*')  # '****Hello World*****'
```

`format()` 함수는 또한 `align`을 쉽게 사용할 수 있습니다. 하고자 하는 모든 것을 `<`, `>`, 또는 `^` 문자와 함께 원하는 너비로 사용할 수 있습니다.

```python
format(text, '>20')    # '         Hello World'
format(text, '<20')    # 'Hello World         '
format(text, '^20')    # '    Hello World     '
```

`space` 보다 문자를 포함하고 싶다면 `alignment character` 앞에 지정하면 됩니다.

```python
format(text, '=>20s')  # '=========Hello World'
format(text, '*^20s')  # '****Hello World*****'
```

이런 형태의 코드는 `format()` 메서드를 여러 변수의 형태로 표현할 때 사용될 수 있습니다.

```python
'{:>10s} {:>10s}'.format('Hello', 'World')
# '     Hello     World'
```
`format()`의 하나의 이점은 문자열을 지정하지 않는다는 것입니다. 어떤 변수와 함께라도 작동하고, 더 일반적인 목적으로 만들 수 있습니다.

```python
x = 1.2345
format(x, '>10')    # '    1.2345'
format(x, '^10.2f)  # '   1.23   '
```

#### Discussion

예전 코드에서는 `% operator`도 `format text`에 사용된 것을 봤을 것입니다.

```python
'%-20s' % text      # 'Hello World         '
'%20s' % text       # '         Hello World'
```

하지만 새로운 코드에서는 `format()` 함수나 메서드를 사용하는 것이 선호되어야 합니다. `format()`이 `% operator`로 제공된 것 보다 훨씬 파워풀합니다. 게다가 `format()`은 좀 더 일반적으로 `ljust()`, `rjust()` 또는 `center()` 문자열 메서드를 어떤 종류의 객체와 함께라도 사용할 수 있습니다.

좀더 완전한 `format()` 함수의 사용 가능한 기능의 목록을 위해, [Python 온라인 문서]()를 참고하시기 바랍니다.

## 2.14 Combining and Concatenating Strings

#### Problem

많은 문자열 조각들을 큰 문자열로 결합하려고 합니다.

#### Solution

결합하려는 문자열이 `sequence` 또는 `iterable` 객체인 경우, 가장 빠른 방법은 `join()` 메서드를 사용하여 결합하는 것입니다.

```python
parts = ['Is', 'Chicago', 'Not', 'Chicago?']
print(' '.join(parts))      # 'Is Chicago Not Chicago?'
print(','.join(parts))      # 'Is,Chicago,Not,Chicago?'
print(''.join(parts))       # 'IsChicagoNotChicago?'
```

첫번째에서는, 문법이 이상해 보이지만, `join() operation`이 문자열의 메서드로 지정되었습니다.
부분적으로 이것은 결합하려는 객체가 임의의 수의 서로 다른 데이터 `sequence`(`list`, `tuple`, `dict`, `file`, `set`, `generator`)로부터 올 수 있고, `join()`을 이런 각각의 모든 객체들에 대해 구현하는 것은 불필요하기 때문입니다. 그러므로 원하는 `seperator string`을 지정하고 텍스트 조각을 붙이기 위해 `join()` 메서드를 사용하기만 하면 됩니다.

만약 아주 조금만 문자열을 결합할 경우 보통 `+` 만 써도 충분합니다

```python
a = 'Is Chicago'
b = 'Not Chicago?'
print(a + ' ' + b)           # 'Is Chicago Not Chicago?'
```

`+ operator` 또한 좀 더 복잡한 문자열 `formatting operation` 대체에 잘 작동합니다.

```python
print('{} {}'.format(a, b))   # 'Is Chicago Not Chicago?'
print(a + ' ' + b)            # 'Is Chicago Not Chicago?'
```

소스 코드에서 문자열 결합을 문자 그대로 결합하려면, `+` 연산자 없이 서로 인접하게 놓을 수 있습니다.

```python
a = 'Hello' 'World'
print(a)                      # 'HelloWorld'
```

#### Discussion

문자열을 서로 `join`하는 것은 전체적인 해결을 보장하기에 충분한 과정으로 보이지는 않지만, 프로그래머가 코드 성능에 심각한 영향을 가진 선택을 하는 영역입니다.

이를 알기위한 가장 중요한 것은 `+` 연산자를 사용하여 많은 문자열을 함께 결합하는 것은 메모리 복사와 `garbage collection`이 일어나기 때문에 매우 비효율적이라는 것입니다.
특히 다음과 같이 문자열을 `join`하려고 해서는 안됩니다.

```python
s = ''
for p in parts:
    s += p
```

이는 `join()` 메서드를 사용하는 것 보다 훨씬 느리게 동작하는데, 주된 이유는 각 `+=`연산자가 새로운 문자열 객체를 만들어 내기 때문입니다. 먼저 모든 부분들을 모은 후에 마지막에 서로 `join`하는 것이 좋습니다.

관련된 하나의 트릭(꽤 산뜻한)은 `generator expression`을 사용하여 데이터를 문자열과 `concatenation`으로 동시에 변환하는 것입니다. [1.19장]()을 참조하시길 바랍니다.

```python
data = ['ACME', 50, 91.1]
print(','.join(str(d) for d in data))   # 'ACME,50,91.1'
```

또한 불필요한 문자열 `concatenation`을 경계하시기 바랍니다. 때론 프로그래머들이 기술적으로 꼭 필요하지 않을 때도 `concatenation`에 휩쓸립니다.

```python
print(a + ':' + b + ':' + c)    # Ugly
print(':'.join([a, b, c]))      # Still Ugly
print(a, b, c, sep=':')         # Better
```

I/O 작업을 섞고 문자열을 `concatenation`하는 것은 애플리케이션에 따른 연구가 필요할 수 있습니다. 예를 들어 다음 두 코드를 고려하면

```python
# Version 1 (string concatenation)
f.write(chunk1 + chunk2)

# Version 2 ( seperate I/O operations)
f.write(chunk1)
f.write(chunk2)
```

두 문자열이 길이가 작다면 첫번째 버전이 I/O 시스템 콜을 수행하는 고유 비용의 이유로 좀 더 나은 성능을 제공합니다. 반면에 두 문자열이 길이가 길다면, 두번째 버전이 좀 더 효율적일 수 있습니다. 이는 많은 일시적인 결과를 만들어내고 큰 메모리 블럭을 복사하는 것을 피할 수 있기 때문입니다.
다시말해, 이것이 최선의 결과를 결정하기 위해 자신의 데이터와 관련되어 연구해야 하는 것임을 강조해야 합니다.

마지막으로, 하지만 적어도 많은 작은 길이의 문자열로 부터 `output`을 만들어내는 코드를 쓰려면 코드 조각을 내보내기 위해 `yield`를 사용한, `generator` 함수를 고려해 보아야 합니다.

```python
def sample():
    yield 'Is'
    yield 'Chicago'
    yield 'Not'
    yield 'Chicago?'
```

흥미로운 점은 이런 접근이 조각이 함께 조립되는 방법에 대한 가정을 만들지 않는다는 것입니다.
예를 들어 `join()`을 사용하여 간단한 코드 조각을 `join`할 수 있습니다.

```python
text = ''.join(sample())
```

또는 조각을 I/O로 `redirect` 시킬 수 있습니다.

```python
for part in sample():
    f.write(part)
```

또는 I/O 작업을 결합하는 것에 대한 영리한 `hybrid scheme`의 종류를 떠올릴 수 있습니다.

```python
def combine(source, maxsize):
    parts = []
    size = 0
    for part in source:
        parts.append(part)
        size += len(part)
        if size > maxsize:
            yield ''.join(parts)
            parts = []
            size = 0
    yield ''.join(parts)

for part in combine(sample(), 32768):
    f.write(part)
```

중요한 점은 원본 `generator` 함수가 정확한 세부 정보를 알 필요 없다는 점입니다. 이는 `yield`의 부분일 뿐입니다.

## 2.15 문자열에서 변수를 넣기

#### Problem

포함된 변수 이름이 변수 값의 문자열 표현으로 대체되는 문자열을 만들려고 합니다.

#### Solution

Python은 문자열에서 간단하게 변수 값을 대체하기 위한 작업을 직접 제공하지 않습니다. 하지만 이 기능은 문자열의 `format()` 메서드를 사용할 때 적절할 수 있습니다.

```python
s = '{name} has {n} messages.'
print(s.format(name='Guido', n=37))     # 'Guido has 37 messages.'
```

또는 값이 모든 변수로 대체되려면 `format_map()`과 `vars()`를 다음과 같이 함께 사용할 수 있습니다.

```python
name = 'Guido'
n = 37
s.format_map(vars())                    # 'Guido has 37 messages.'
```

`vars()`의 미묘한 기능 중 하나는 인스턴스에서도 작동한다는 것입니다.

```python
class Info:
    def __init__(self, name, n):
        self.name = name
        self.n = n

a = Info('Guido', 37)
print(s.format_map(vars(a)))            # 'Guido has 37 messages.'
```

`format()`과 `format_map()`의 한가지 단점은 누락된 값을 정상적으로 처리하진 못한다는 것입니다.

```python
s.format(name='Guido')                  # KeyError: 'n'
```

이를 피하는 한가지 방법은 대안으로 `dictionary` 클래스에 `__missing__()` 메서드를 함께 정의하는 것입니다.

```python
class Safesub(dict):
    def __missing__(self, key):
        return '{' + key + '}'
```

이제 이 클래스를 `format_map()`에 `input`을 `wrapping` 하기 위해 사용할 수 있습니다.

```python
del n                                   # Make sure n is undefined
print(s.format_map(Safesub(vars())))    # 'Guido has {n} messages.'
```

이러한 단계를 자주 수행하는 경우 소위 `frame hack`이라 불리는 변수를 대체하는 프로세스 뒤에 작은 유틸리티 함수를 숨길 수 있습니다.

```python
import sys
def sub(text):
    return text.format_map(Safesub(sys._getframe(1).f_locals))

name = 'Guido'
n = 37
print(sub('Hello {name}'))                      # Hello Guido
print(sub('You have {n} messages.'))            # You have 37 messages.
print(sub('Your favorite color is {color}'))    # Your favorite color is {color}
```

#### Discussion

Python에선 변수에 넣는 방법이 부족하기 때문에 다양한 해결책이 오랫동안 개발되어 왔습니다. 여기서 소개된 방법의 대안으로, 때로는 다음과 같이 문자열을 `formatting` 할 수 있음을 볼 수 있습니다.

```python
name = 'Guido'
n = 37
print('%(name) has %(n) messages.' % vars())    # 'Guido has 37 messages.'
```

또한 템플릿 문자열을 사용할 수도 있습니다.

```python
import string
s = string.Template('$name has $n messages.')
print(s.substitute(vars()))                     # 'Guido has 37 messages.'
```

하지만 `format()`과 `format_map()` 메서드는 이런 대안들 중 하나보다는 좀 더 최신 버전이므로 선호되어야 합니다. `format()`의 이점 중 하나는 간단한 `template` 문자열 객체와 같은 대안으로는 불가능한 문자열 `formatting`과 관련된 모든 기능(`alignment`, `padding`, `numerical formatting` 등)을 얻을 수 있다는 것입니다.

이 해결책의 일부는 또한 매우 흥미로운 고급 기능들을 보여줍니다. 조금 알려진 `mapping/dict` 클래스의 `__missing__()` 메서드는 빠뜨린 값의 `handling`을 정의할 수 있는 메서드입니다. `Safesub` 클래스에서 이 메서드는 빠뜨린 값을 `placeholder`로써 반환하기 위해 정의되었습니다.
`KeyError` 예외를 얻게 되는 대신, 결과 문자열에서 빠뜨린 값이 나타나는 것을 볼 수 있게 될 것입니다. (디버깅에도 유용할 수 있습니다.)

`sub()` 함수는 `caller`의 스택 프레임을 반환하기 위해 `sys._getframe(1)`을 사용합니다. 이로써 `f_locals` 속성에 액세스하여 로컬 변수를 얻게 됩니다. 대부분의 코드에서 스택 프레임을 어지럽히는 것을 피해야 한다는 것은 말할 필요도 없습니다. 하지만 문자열 대체 기능 같은 유틸리티 함수를 위해서는 유용할 수 있습니다. 각설하고, `f_locals`은 함수 호출에서 로컬 변수를 복사하는 `dictionary`로써 주목할 만한 가치가 있습니다.
`f_locals`의 내용을 수정할 수는 있지만, 수정은 실제로 지속적인 영향을 주지는 않습니다. 그러므로 다른 스택 프레임에 액세스 하는것이 안 좋게 보이더라도 실수로 변수를 덮어쓰거나 `caller`의 로컬 환경을 변경할 수는 없습니다.

## 2.16 Reformatting Text to a Fixed Number of Columns

#### Problem

긴 문자열에서 사용자가 지정한 컬럼 수를 채울 수 있도록 `reformat` 하려고 합니다.

#### Solution

`output`을 위한 텍스트 `reformat`을 위한 `textwrap` 모듈을 사용합니다. 다음과 같은 긴 문자열을 가정합니다.

```python
s = "Look into my eyes, look into my eyes, the eyes, the eyes, \
the eyes, not around the eyes, don't look around the eyes, \
look into my eyes, you're under."
```

여기서 `textwrap` 모듈을 사용하여 다양한 방법으로 `reformat` 할 수 있습니다.

```python
>>> import textwrap
>>> print(textwrap.fill(s, 70))
Look into my eyes, look into my eyes, the eyes, the eyes, the eyes,
not around the eyes, don't look around the eyes, look into my eyes,
you're under.
>>> print(textwrap.fill(s, 40))
Look into my eyes, look into my eyes,
the eyes, the eyes, the eyes, not around
the eyes, don't look around the eyes,
look into my eyes, you're under.
>>> print(textwrap.fill(s, 40, initial_indent='    '))
    Look into my eyes, look into my
eyes, the eyes, the eyes, the eyes, not
around the eyes, don't look around the
eyes, look into my eyes, you're under.
>>> print(textwrap.fill(s, 40, subsequent_indent='    '))
Look into my eyes, look into my eyes,
    the eyes, the eyes, the eyes, not
    around the eyes, don't look around
    the eyes, look into my eyes, you're
    under.
```

#### Discussion

`textwrap` 모듈은 텍스트를 출력용으로 정리하기 위한, 특히 터미널에 잘 맞도록 할 때의 간단한 방법입니다. 터미널 크기에 대해서는 `os.get_terminal_size()`을 사용함으로써 얻을 수 있습니다.

```python
import os
print(os.get_terminal_size().columns)       # 80
```

`fill()` 메서드는 `tab`, `sentence ending` 등을 `handle`하는 방법을 제어하는 몇 가지 추가적인 옵션을 가지고 있습니다. [textwrap.TextWrapper 클래스 문서](//docs.python.org/3.3/library/textwrap.html#textwrap.TextWrapper)에서 자세한 내용을 참조하시길 바랍니다.

## 2.17 Handling HTML and XML Entities in TextWrapper

#### Problem

`&entity;` 또는 `&#code;`와 대응되는 텍스트와 같이 HTML 또는 XML 항목을 대체하고 싶습니다.
대안으로, 텍스트를 생성하지만, `<`, `>`, `&` 같은 특정 문자들을 `escape` 해야 합니다.

#### Solution

텍스트를 생성할 때 `html.escape()` 함수를 사용할 경우 `<`, `>` 같은 특수 문자를 대체하는 것은 상대적으로 쉽습니다.

```python
s = 'Elements are written as "<tag>text</tag>".'
import html
print(s)                # Elements are written as "<tag>text</tag>".
print(html.escape(s))   # Elements are written as &quot;&lt;tag&gt;text&lt;/tag&gt;&quot;.

# Disable escaping of quotes
print(html.escape(s, quote=False))  # Elements are written as "&lt;tag&gt;text&lt;/tag&gt;".
```

만일 ASCII 같은 텍스트로 내보내고 non-ASCII 문자를 위한 문자 코드 항목을 포함 하려면
다양한 I/O 관련 함수를 위한 `errors='xmlcharrefreplace'` 인수를 사용할 수 있습니다.

```python
s = 'Spicy Jalapeño'
print(s.encode('ascii', errors='xmlcharrefreplace'))    # b'Spicy Jalape&#241;o'
```

텍스트에서 항목을 대체하려면 다른 접근이 필요합니다. 실제로 HTML이나 XML을 처리할 경우 HTML 또는 XML 파서를 먼저 적절히 사용하시길 바랍니다. 일반적으로, 이런 도구들은 파싱할 동안 자동으로 값을 바꾸어 처리하므로 이에 신경을 쓰지 않아도 됩니다.

만일 어떤 이유로 항목이 포함된 `bare text`를 받았는데 이를 수동으로 바꾸고 싶다면, HTML 또는 XML 파서와 연관된 다양한 유틸리티 함수/메서드를 사용할 수 있습니다.

```python
s = 'Spicy &quot;Jalape&#241;o&quot.'
from html.parser import HTMLParser
p = HTMLParser()
print(p.unescape(s))    # 'Spicy "Jalapeño".'

t = 'The prompt is &gt;&gt;&gt;'
from xml.sax.saxutils import unescape
print(unescape(t))      # 'The prompt is >>>'
```

#### Discussion

특수 문자의 적절한 `escaping`은 HTML 또는 XML 생성에 대한 세부적인 것을 간과하기 쉽습니다.
이는  `output`을 `print()` 또는 다른 기본 문자열 `formatting` 기능을 사용하여 직접 생성할 경우 특히 그렇습니다. `html.escape()` 같은 유틸리티 함수를 사용하는 것이 쉬운 해결책이 됩니다.

만약 다른 방향으로 텍스트를 처리할 필요가 있다면, `xml.sax.saxutils.unescape()`같은 다양한 유틸리티 함수가 도움이 될 수 있습니다. 하지만 적절한 파서의 사용을 꼭 조사해야 합니다. 예를 들면, HTML 또는 XML 처리에서, `html.parser` 또는 `xml.etree.ElementTree` 같은 파싱 모듈을 사용하는 것은 이미 입력 텍스트에서 항목을 바꾸는 것과 관련된 세부 정보가 처리되어 있어야 합니다.

## 2.18 Tokenizing Text

#### Problem

왼쪽 에서 오른쪽으로 토큰 스트림으로 `parse`할 문자열이 있습니다.

#### Solution

다음과 같은 문자열을 가정합니다.

```python
text = 'foo = 23 + 42 * 10'
```

문자열을 토큰화하기 위해 단순히 패턴을 매칭하는 것 이상이 필요합니다. 패턴 종류를 잘 식별하는 방법이 필요합니다. 예를 들어 이 문자열을 다음과 같은 쌍의 `sequence`로 바꿀 수도 있습니다.

```python
tokens = [('NAME', 'foo'), ('EQ', '='), ('NUM', '23'), ('PLUS', '+'), ('NUM', '42'), ('TIMES', '*'), ('NUM', 10)]
```

이런 종류의 분할을 수행하려면, 첫번째 단계는 모든 가능한 토큰을 `whitespace`를 포함하여, 다음과 같은 명명된  `capture group`을 사용하는 정규 표현식 패턴으로써 정의하는 것입니다.

```python
import re
NAME = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'
NUM = r'(?P<NUM>\d+)'
PLUS = r'(?P<PLUS>\+)'
TIMES = r'(?P<TIMES>\*)'
EQ = r'(?P<EQ>=)'
WS = r'(?P<WS>\s+)'

master_pat = re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS]))
```

이 `re` 패턴에서 `?P<TOKENNAME>` 규칙은 패턴에 이름을 지정하는 데 사용됩니다. 이는 나중에 다루게 됩니다.

다음으로, 토큰화를 위해 조금 알려진 패턴 객체의 `scanner()` 메서드를 사용합니다. 이 메서드는 한번에 하나씩 일치하는 텍스트를 제공함으로써 `match()`단계의 호출을 반복하는 스캐너 객체를 만듭니다. 여기 스캐너 객체가 작동하는 방식에 대한 대화식 예제가 있습니다.

```python
>>> scanner = master_pat.scanner('foo = 42')
>>> scanner.match()
<_sre.SRE_Match object; span=(0, 3), match='foo'>
>>> _.lastgroup, _.group()
('NAME', 'foo')
>>> scanner.match()
<_sre.SRE_Match object; span=(3, 4), match=' '>
>>> _.lastgroup, _.group()
('WS', ' ')
>>> scanner.match()
<_sre.SRE_Match object; span=(4, 5), match='='>
>>> _.lastgroup, _.group()
('EQ', '=')
# 매칭된 객체는 한번 호출 되면 사라집니다.
```

이 테크닉을 취하고 코드에 넣으려면, 다음과 같이 `generator`에 정리 하고 쉽게 패키지화 할 수 있습니다.

```python
from collections import namedtuple

Token = namedtuple('Token', ['type', 'value'])

def generate_tokens(pat, text):
    scanner = pat.scanner(text)
    for m in iter(scanner.match, None):
        yield Token(m.lastgroup, m.group())

# Example use
for tok in generate_tokens(master_pat, 'foo = 42'):
    print(tok)

# Produces output
# Token(type='NAME', value='foo')
# Token(type='WS', value=' ')
# Token(type='EQ', value='=')
# Token(type='WS', value=' ')
# Token(type='NUM', value='42')
```

만일 토큰 스트림을 어떤 식으로든 `filter`하고 싶으면 더 많은 `generator` 함수나 `generator expression`을 사용하여 정의할 수 있습니다. 예를 들어 다음은 모든 `whitespace` 토큰을 `filter` 하는 방법입니다.

```python
tokens = (tok for tok in generate_tokens(master_pat, text) if tok.type != 'WS')
for tok in tokens:
    print(tok)
```

#### Discussion

토큰화한다는 것은 좀 더 발전된 텍스트 `parsing`과 `handling`을 위한 첫 단계입니다.
위의 `scanning` 테크닉을 사용하려면, 기억해야 할 매우 중요한 세부 사항이 있습니다.
첫째로, `re` 패턴에 대응하는 `input`을 나타낼 수 있는 모든 가능한 `sequence` 식별을 확실히 해야 합니다. 어떤 매칭안된 텍스트가 발견 되었다면 `scanning`은 단순히 정지합니다. 이는 `whitespace(WS)` 토큰이 예제에서 지정되는 데 필요한 이유입니다.

`master 정규 표현식`의 토큰의 순서 역시 문제입니다. 매칭 될 때 `re`는 지정된 순서에서 패턴 매치를 시도합니다. 그러므로 패턴이 긴 패턴의 `substring`이 될 경우 긴 패턴을 먼저 오도록 해야 합니다.

```python
LT = r'(?P<LT><)'
LE = r'(?P<LE><=)'
EQ = r'(?P<EQ>=)'

master_pat = re.compile('|'.join([LE, LT, EQ]))     # Correct
# master_pat = re.compile('|'.join([LT, LE, EQ]))   # Incorrect
```

두번째 패턴은 토큰 `LT` 뒤에 `LE`가 아니라 `EQ`가 뒤에 따라오는 `<=`와 매치되기 때문에 틀린 표현입니다.

마지막으로 하지만 적어도 `substring` 형태의 패턴을 조심해야 할 필요가 있습니다. 예를 들어 다음 두 패턴이 있다고 가정하면

```python
PRINT = r'(?P<PRINT>print)'
NAME = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'

master_pat = re.compile('|'.join([PRINT, NAME]))

for tok in generate_tokens(master_pat, 'printer'):
    print(tok)

# Outputs:
# Token(type='PRINT', value='print')
# Token(type='NAME', value='er')
```

토큰화의 좀더 자세한 종류는 [PyParsing](//pyparsing.wikispaces.com/) 또는 [PLY](//www.dabeaz.com/ply/index.html) 같은 패키지를 통해 확인 하실 수 있습니다. 다음 챕터에 PLY에 대한 예제가 포함됩니다.

## 2.19 Writing a Simple Recursive Descent Parser

#### Problem

문법 규칙의 집합에 따라 텍스트를 파싱하고 작업을 수행하거나 입력을 나타내는 추상 `syntax tree`를 작성해야 합니다. 문법은 규모가 작기 때문에 어떤 종류의 프레임워크를 사용하는 것과는 반대로 직접 `parser`를 작성하는 것이 좋습니다.

#### Solution

이 문제에는 우리가 특정 문법에 따라 텍스트를 파싱하는 문제에 중점을 둡니다. 이를 위해서 BNF 또는 EBNF 형태의 문법의 형식 지정을 시작해야 할 것입니다. 예를 들어 간단한 산술 표현식 문법을 다음과 같이 나타낼 수 있습니다.

```
expr ::= expr + term
     |  expr - term
     |  term
term ::= term * factor
     |  term / factor
     |  factor
factor ::= ( expr )
       |    NUM
```

또는 EBNF 형태로 나타냅니다.

```
expr ::= term { (+|-) term }*
term ::= factor { (*|/) factor }*
factor ::= ( expr )
       |    NUM
```

EBNF에서 `{ ... }*`로 둘러싸인 부분의 규칙은 생략 가능하다는 뜻입니다. `*`의 의미는 0 혹은 그 이상의 반복을 나타냅니다.(정규 표현식에서와 의미가 같습니다.)

이제, BNF가 돌아가는 원리에 대해 친숙하지 않다면, 대체 규칙을 지정한다거나 또는 왼쪽의 기호가 오른쪽의 기호로 바뀌는 대체 규칙으로 생각해 보시길 바랍니다.(반대의 경우도 마찬가지입니다.)
일반적으로 파싱하는 동안에 일어나는 일은 `input` 텍스트를 BNF를 통한 다양한 대체 변수와 확장을 만듦으로써 문법에 매칭하려는 것입니다. 예를 들면 `3 + 4 * 5`와 같은 표현식을 파싱한다고 가정합니다. 이 표현식은 [2.18장]()에 소개된 기술을 이용하여 먼저 토큰 스트림으로 쪼개야 합니다. 토큰의 `sequence`는 다음과 같이 나타낼 수 있습니다.

```
NUM + NUM * NUM
```

거기서 파싱은 대체 생성을 통한 `input` 토큰의 문법 매치를 시도하는 것을 포함합니다.

```
# 이는 수학 문제를 풀 듯이 위에서 정의한 대로 같은 문법을 찾아가면서 expr ::= NUM + NUM * NUM 형태를 찾아 나가는 과정을 보여주는 것입니다.
expr
expr ::= term { (+|-) term }*
expr ::= factor { (*|/) factor }* { (+|-) term }*
expr ::= NUM { (*|/) factor }* { (+|-) term }*
expr ::= NUM { (+|-) term }*
expr ::= NUM + term { (+|-) term }*
expr ::= NUM + factor { (*|/) factor }* { (+|-) term }*
expr ::= NUM + NUM { (*|/) factor }* { (+|-) term }*
expr ::= NUM + NUM * factor { (*|/) factor }* { (+|-) term }*
expr ::= NUM + NUM * NUM { (*|/) factor}* { (+|-) term }*
expr ::= NUM + NUM * NUM { (+|-) term }*
expr ::= NUM + NUM * NUM
```

모든 대체 단계를 따라 가려면 약간의 커피가 필요하지만 `input`에서 문법 규칙에 매치하려는 것으로 이끌어 냅니다. 첫 `input` 토큰은 `NUM`이라서 대체는 먼저 해당 부분과 매칭에 초점을 둡니다. 한번 매칭이 되면 작업은 다음 `+` 등의 토큰으로 이동합니다. 오른쪽 측면의 특정 부분(예: `{ (*|/) factor }*`)은 다음 토큰이 매치될 수 없다고 결정될 때 사라집니다. `parse`가 성공하면 전체 오른쪽 측면이 `input` 토큰 스트림에 매치하기 위해 완전히 확장됩니다.

모든 백그라운드 작업이 갖추어지면, 이제 간단하게 `recursive descent expression evaluator`를 만드는 방법을 보여줍니다.

```python
import re
import collections

# Token specification
NUM = r'(?P<NUM>\d+)'
PLUS = r'(?P<PLUS>\+)'
MINUS = r'(?P<MINUS>-)'
TIMES = r'(?P<TIMES>\*)'
DIVIDE = r'(?P<DIVIDE>/)'
LPAREN = r'(?P<LPAREN>\()'
RPAREN = r'(?P<RPAREN>\))'
WS = r'(?P<WS>\s+)'

master_pat = re.compile('|'.join([NUM, PLUS, MINUS, TIMES, DIVIDE, LPAREN, RPAREN, WS]))

# Tokenizer
Token = collections.namedtuple('Token', ['type', 'value'])

def generate_tokens(text):
    scanner = master_pat.scanner(text)
    for m in iter(scanner.match, None):
        tok = Token(m.lastgroup, m.group())
        if tok.type != 'WS':
            yield tok

# parser
class ExpressionEvaluator:
    '''
    Implementation of a recursive descent parser.
    Each method implements a single grammar rule.
    Use the ._accept() method to test and accept the current lookahead token.
    Use the ._expect() method to exactly match and discard the next token on on the input (or raise a SyntaxError if it doesn't match).
    '''

    def parse(self, text):
        self.tokens = generate_tokens(text)
        self.tok = None         # Last symbol consumed
        self.nexttok = None     # Next symbol tokenized
        self._advance()         # Load first lookahead token
        return self.expr()

    def _advance(self):
        'Advance one token ahead'
        self.tok, self.nexttok = self.nexttok, next(self.tokens, None)

    def _accept(self, toktype):
        'Test and consume the next token if it matches toktype'
        if self.nexttok and self.nexttok.type == toktype:
            self._advance()
            return True
        else:
            return False

    def _expect(self, toktype):
        'Consume next token if it matches toktype or raise SyntaxError'
        if not self.accept(toktype):
            raise SyntaxError('Expected ' + toktype)

    # Grammar rules follow
    def expr(self):
        "expression ::= term { ('+' | '-') term }*"
        exprval = self.term()
        while self._accept('PLUS') or self._accept('MINUS'):
            op = self.tok.type
            right = self.term()
            if op == 'PLUS':
                exprval += right
            elif op == 'MINUS':
                exprval -= right
        return exprval

    def term(self):
        "term ::= factor { ('*' | '/') factor }*"
        termval = self.factor()
        while self._accept('TIMES') of self._accept('DIVIDE'):
            op = self.tok.type
            right = self.factor()
            if op == 'TIMES':
                termval *= right
            elif op == 'DIVIDE':
                termval /= right
        return termval

    def factor(self):
        "factor ::= NUM | ( expr )"
        if self._accept('NUM'):
            return int(self.tok.value)
        elif self._accept('LPAREN'):
            exprval = self.expr()
            self._expect('RPAREN')
            return exprval
        else:
            raise SyntaxError('Expected NUMBER or LPAREN')
```

`ExpressionEvaluator` 클래스를 대화식으로 사용하는 예제입니다.

```python
# 보시면 string 객체에 대해서도 우선순위가 정확히 적용됨을 알 수 있습니다.
>>> e = ExpressionEvaluator()
>>> e.parse('2')
2
>>> e.parse('2 + 3')
5
>>> e.parse('2 + 3 * 4')
14
>>> e.parse('2 + (3 + 4) * 5')
37
>>> e.parse('2 + (3 + * 4)')
SyntaxError: Expected Number or LPAREN
```

순수한 `evaluation`이 아닌 다른 것을 원한다면 `ExpressionEvaluator` 클래스를 바꾸어 다른 것을 수행해야 합니다. 예를 들어 간단한 `parse tree`를 구성하는 구현의 다른 방법입니다.

```python
class ExpressionTreeBuilder(ExpressionEvaluator):
    def expr(self):
        "expression ::= term { ('+'|'-') term }*"
        exprval = self.term()
        while self._accept('PLUS') or self._accpet('MINUS'):
            op = self.tok.type
            right = self.term()
            if op == 'PLUS':
                exprval = ('+', exprval, right)
            elif op == 'MINUS':
                exprval = ('-', exprval, right)
        return exprval

    def term(self):
        "term ::= factor { ('*'|'/') factor }*"
        termval = self.factor()
        while self._accept('TIMES') or self._accept('DIVIDE'):
            op = self.tok.type
            right = self.factor()
            if op == 'TIMES':
                termval = ('*', termval, right)
            elif op == 'DIVIDE':
                termval = ('/', termval, right)
        return termval

    def factor(self):
        "factor ::= NUM | ( expr )"
        if self._accept('NUM'):
            return int(self.tok.value)
        elif self._accept('LPAREN'):
            exprval = self.expr()
            self._expect('RPAREN')
            return exprval
        else:
            raise SyntaxError('Expected NUMBER or LPAREN')
```

다음 예제는 어떻게 하는지를 보여줍니다.

```python
e = ExpressionTreeBuilder()
>>> e.parse('2 + 3')
('+', 2, 3)
>>> e.parse('2 + 3 * 4')
('+', 2, ('*', 3, 4))
>>> e.parse('2 + (3 + 4) * 5')
('+', 2, ('*', ('+', 3, 4), 5))
>>> e.parse('2 + 3 + 4')
('+', ('+', 2, 3), 4)
```

#### Memo

코드가 매우 깔끔합니다. 컴파일러같이 BNF를 이용하여 설계하는 점이 흥미롭고 설계한 대로 정확하게 구현된 모습입니다.

이 예제는 스트링 객체에 문법이란 것을 매칭하여 마치 실제 숫자 객체에서의 우선순위를 그대로 매핑한 것이며 문법을 **사용자 정의**로 구현할 수 있다는게 매우 흥미로운 점인 것 같습니다.

조금만 응용하면 자연어 처리의 형태소를 분석한다거나 진짜 **나만의 파서**를 만드는데 사용할 수도 있을 것 같다는 생각이 듭니다.

## Discussion

파싱은 일반적으로 컴파일러 과목의 첫 3주를 차지하는 거대한 주제입니다. 문법, 파싱과 다른 정보에 관한 배경지식을 찾고 있다면 컴파일러 책을 참조해야 합니다. 말할 필요도 없이 모든 과정을 여기서 되풀이 할 수는 없습니다.

그럼에도 불구하고 `recursive descent parser`를 만드는 전반적인 개념은 일반적으로 간단합니다.
시작하려면, 모든 문법 규칙을 취하고 함수 또는 메서드로 바꿉니다. 따라서 문법이 다음과 같이 보이면

```
expr ::= term { ('+'|'-') term }*
term ::= factor { ('*'|'/') factor }*
factor ::= '(' expr ')'
        | NUM
```

이것을 다음과 같이 바꾸는 것으로 시작합니다.

```python
class ExpressionEvaluator:

    def expr(self):
        pass

    def term(self):
        pass

    def factor(self):
        pass
```

각 메서드의 작업은 간단합니다. 왼쪽에서 오른쪽으로 문법 규칙의 각 부분을 진행하고 프로세스에서 토큰을 소비해야합니다. 어떤 의미에서, 메서드의 목표는 규칙을 사용하거나 멈출 때 `syntax error`를 발생시키는 것입니다. 이를 위해 다음 구현 기술이 적용됩니다.

* 규칙에서 다음 기호가 또다른 문법 규칙(예: `term` 또는 `factor`)의 이름이라면 간단히 동일한 이름의 메서드를 호출하면 됩니다. 이것이 알고리즘의 `descent` 부분이 다른 문법의 규칙으로 하강하는 것을 의미합니다. 때로는 규칙이 이미 실행중인 메서드에 대한 호출을 포함할 수 있습니다.
(예: `factor ::= '(' expr ')'`에서의 `expr` 호출) 이는 알고리즘의 `recursive`한 부분입니다.

* 규칙에서의 다음 기호가 특정 기호가 될 경우(예: `(`)에는 다음 토큰을 보고 정확한 매치를 위한 체크를 합니다. 매치가 되지 않으면 `syntax error`입니다. `_expect()` 메서드는 이 예제에서 이런 단계를 수행하기 위해 사용되었습니다.

* 규칙에서의 다음 기호에서 선택이 가능하다면(예: `+` 또는 `-`) 각 가능성에 대해 다음 토큰을 체크해야 하고 일치하는 경우에만 진행합니다.
이는 이 장에서 `_accept()` 메서드가 가진 목적입니다. 이는 매치가 될 경우 `_except()` 메서드의 약한 버전같이 진행되지만, 그렇지 않을 경우에는 오류를 발생시키지는 않고 간단히 뒤로 진행합니다. (그러므로 더 많은 체크를 할 수 있게됩니다.)

* 문법 규칙에서 되풀이 되는 부분(`expr ::= term { ('+'|'-') term }*` 규칙과 같은)의 반복은 `while` 루프에 의해 구현됩니다.
루프의 `body`는 일반적으로 모든 반복된 항목을 더이상 찾을 수 없을 때 까지 모으거나 처리할 것입니다.

* 한번 전체적인 문법 규칙이 쓰여지면, 각 메서드는 어떤 종류의 결과를 `caller`에 반환합니다. 이는 변수가 파싱될동안 진행되는 방법입니다. 예를 들어 `expression evaluator`에서 반환된 변수는 파싱되는 표현식의 부분적인 결과로 표현될 것입니다. 결국 이들 모두가 실행되는 최상위 문법 규칙 메서드로 함께 결합됩니다.

비록 간단한 예제를 보여줬지만, `recursive descent parser`는 좀 덜 복잡한 파서의 구현에도 사용 될 수 있습니다. 예를 들어, Python 코드 자체가 `recursive descent parser`로 해석하는 것입니다. 만일 그럴 경향이 있다면 Python 소스의 문법/문법 파일의 검사함으로써 기본 문법을 볼 수 있습니다. 그렇다고 해도 손으로 파서를 만드는 데는 여전히 여러 함정과 한계가 있습니다.

`recursive descent parser`의 한가지 한계점은 모든 왼쪽의 재귀 종류를 포함할 문법 규칙을 위해 쓰일 수 없다는 것입니다. 예를 들어 다음과 같은 규칙을 `translate`한다고 하면

```
items ::= items ',' item
        | item
```

이것을 하려면 `items()` 메서드를 다음과 같이 사용하려 할 것입니다.

```python
def items(self):
    itemsval = self.items()
    if itemsval and self._accept(','):
        itemsval.append(self.item())
    else:
        itemsval = [ self.item() ]
```

문제는 이것이 동작하지 않는 다는 것입니다. 사실은, 무한 재귀 에러가 발생하게 됩니다.

문법규칙들 자체를 고려하는 까다로운 문제 또한 실행할 수 있습니다. 예를 들어 표현식을 간단한 문법으로써 설명할 수 있는지 아닌지 궁금하다면

```
expr ::= factor { ('+'|'-'|'*'|'/') factor }*
factor ::= '(' expression ')'
        | NUM
```

이 문법은 기술적으로 동작은 하지만 계산 순서를 고려하는 표준 산술 규칙을 관찰할 수 없습니다. `3 + 4 * 5`라는 표현식은 23이라는 결과를 기다한 것과는 달리 `35`라는 값을 얻게됩니다. `expr` 과 `term`규칙을 각각 사용하는 것이 정확한 계산을 하게 됩니다.

좀더 복잡한 문법을 위해서는 [PyParsing](//pyparsing.wikispaces.com/)이나 [PLY](//www.dabeaz.com/ply/index.html) 파싱 툴을 사용하는 게 나을 수 있습니다. 다음은 PLY를 사용하여 `evaluator` 코드를 표현한 것입니다.

```python
from ply.lex import lex
from ply.yacc import yacc

# Token list
tokens = ['NUM', 'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'LPAREN', 'RPAREN']

# Ignored characters
t_ignore = ' \t\n'

# Token specifications (as regexs)
t_PLUS = r'\+'
t_MINUS = r'-'
t_TIMES = r'\*'
t_DIVIDE = r'/'
t_LPAREN = r'\('
t_RPAREN = r'\)'

# Token processing functions
def t_NUM(t):
    "r'\d+'"
    t.value = int(t.value)
    return t

# Errpr handler
def t_error(t):
    print('Bad character: {!r}'.format(t.value[0]))
    t.skip(1)

# Build the lexer
lexer = lex()

# Grammar rules and handler functions
def p_expr(p):
    '''
    expr : expr PLUS term
        | expr MINUS term
    '''
    if p[2] == '+':
        p[0] = p[1] + p[3]
    elif p[2] == '-':
        p[0] = p[1] - p[3]


def p_expr_term(p):
    '''
    expr : term
    '''
    p[0] = p[1]

def p_term(p):
    '''
    term : term TIMES factor
        | term DIVIDE factor
    '''
    if p[2] == '*':
        p[0] = p[1] * p[3]
    elif p[2] == '/':
        p[0] = p[1] / p[3]

def p_term_factor(p):
    '''
    term : factor
    '''
    p[0] = p[1]

def p_factor(p):
    '''
    factor : NUM
    '''
    p[0] = p[1]

def p_factor_group(p):
    '''
    factor : LPAREN expr RPAREN
    '''
    p[0] = p[2]

def p_error(p):
    print('Syntax error')

parser = yacc()
```

이 코드에서 모든 것이 더 높은 레벨로 지정되었다는 것을 알 수 있습니다.
간단하게 다양한 문법 규칙에 매치될 때 실행되는 토큰과 높은 레벨의 핸들링 함수에 대한 정규 표현식을 작성하기만 하면 됩니다.
파서 실행하고 토큰을 받는 등의 실제 원리는 전체적으로 라이브러리에 의해 구현되는 것입니다.

파서 객체가 사용된 결과를 얻는 방법의 예제입니다.

```python
>>> parser.parse('2')
2
>>> parser.parse('2+3')
5
parser.parse('2+(3+4)*5')
37
```

프로그래밍에 좀 더 흥미가 필요하다면 파서와 컴파일러 작성은 재미있는 프로젝트가 될 수 있습니다. 다시말해 컴파일러 책은 많은 `low-level` 세부사항에 대한 이론을 가질 것입니다. 하지만 많은 좋은 리소스가 온라인에서도 찾을 수 있습니다. Python 자체의 `ast` 모듈 역시 볼 가치가 있습니다.

## 2.20 바이트 문자열의 텍스트 Operation 수행

#### Problem

바이트 문자열에서 일반적인 텍스트 연산(`stripping`, `searching`, `replacement`)을 수행하고 싶습니다.

#### Solution

바이트 문자열은 텍스트 문자열에서 `built-in`연산의 대부분에서 이미 지원합니다. 예를 들면

```python
>>> data = b'Hello World'
>>> data[0:5]
b'Hello'
>>> data.startswith(b'Hello')
True
>>> data.split()
[b'Hello', b'World']
>>> data.replace(b'Hello', b'Hello Cruel')
b'Hello Cruel World'
```

어떤 연산 또한 바이트 배열과 함께 작동합니다.

```python
>>> data = bytearray(b'Hello World')
>>> data[0:5]
bytearray(b'Hello')
>>> data.startswith(b'Hello')
True
>>> data.split()
[bytearray(b'Hello'), bytearray(b'World')]
>>> data.replace(b'Hello', b'Hello Cruel')
bytearray(b'Hello Cruel World')
```

바이트 문자열을 위한 정규 표현식 패턴 매칭도 적용할 수 있지만 패턴 자체가 바이트로 지정되어야 합니다. 예를 들면

```python
>>> data = b'FOO:BAR,SPAM'
>>> import re
>>> re.split('[:,]', data)
TypeError: Can't use a string pattern on a bytes-like object

>>> re.split(b'[:,]', data)     # Notice: pattern as bytes
[b'FOO', b'BAR', b'SPAM']
```

#### Discussion

대부분에서 텍스트 문자열에서 연산이 가능한 거의 모든 것은 바이트 문자열에서 동작합니다. 하지만 주의해야할 근소한 차이가 있습니다. 첫째로 바이트 문자열의 `indexing`은 각각의 문자가 아니라 `integer`를 생성합니다.

```python
>>> a = 'Hello World'       # Text string
>>> a[0]
'H'
>>> a[1]
'e'
>>> b = b'Hello World'      # Byte string
>>> b[0]
72
>>> b[1]
101
```

이 의미 차이는 문자 단위 기반에서 바이트 기반의 데이터 처리를 시도하는 프로그램에 영향을 줄 수 있습니다.

두번째는 바이트 문자열은 좋은 문자열 표현을 제공하지 않고 먼저 텍스트로 디코딩되지 않으면 정리된 출력을 하지 않는다는 것입니다.

```python
>>> s = b'Hello World'
>>> print(s)
b'Hello World'
>>> print(s.decode('ascii'))
Hello World
```

비슷하게, 바이트 문자열에 가능한 `string formatting operation`이 없습니다.

```python
>>> b'%10s %10d %10.2f' % (b'ACME', 100, 490.1)
TypeError: unsupported operand type(s) for %: 'bytes' and 'tuple'

>>> b'{} {} {}'.format(b'ACME', 100, 490.1)
AttributeError: 'bytes' object has no attribute 'format'
```

바이트 문자열을 위해 어떤 `formatting` 종류를 적용하려면 일반적인 텍스트 문자열과 인코딩을 사용하여 만들어야 합니다.

```python
>>> '{:10s} {:10d} {:10.2f}'.format('ACME', 100, 490.1).encode('ascii')
b'ACME              100     490.10'
```

마지막으로 바이트 문자열을 사용하면 특정 연산, 특히 파일시스템과 연관된 의미로 변할 수 있다는 것 주의해야합니다. 예를 들어 바이트로 인코딩 된 파일 이름을 텍스트 문자열 대신 제공하려면 보통 파일이름 인코딩/디코딩을 `disable`합니다.

```python
>>> # Write a UTF-8 filename
>>> with open('jalape\xf1o.txt', 'w') as f:
...     f.write('spicy')
...

>>> # Get a dictionary listing
>>> import os
>>> os.listdir('.')         # Text string (names are decoded)
['jalapeño.txt']
>>> os.listdir(b'.')        # Byte string (names left as bytes)
[b'jalapen\xcc\x83o.txt']
```

이 예제의 마지막 부분에서 바이트 문자열이 디렉터리 이름으로 주어지면 파일 이름이 디코딩되지 않은 바이트로 반환되는 데 주목하시길 바랍니다.
디렉터리에서 파일 이름은 `raw UTF-8` 인코딩을 포함한 목록으로 보여집니다. [5.15장]()을 참조하여 파일이름에 관련된 문제를 볼 수 있습니다.

마지막으로 어떤 프로그래머들이 성능 향상 가능성 때문에 텍스트 문자열을 대체하는 것 같은 바이트 문자열을 사용하려는 경향이 있습니다. 비록 (유니코드와 관련된 본질적인 오버헤드로 인해) 바이트 조작이 텍스트 보다 약간 효율적인 경향이 있다는 것은 맞는 말이지만 일반적으로 그렇게 하면 복잡하고 비기본적인(nonidiomatic) 코드가 발생합니다. 바이트 문자열은 Python의 다른 많은 부분에서 잘 작동하지 않으며 올바르게 동작하기 위해 모든 종류의 인코딩 디코딩 작업을 결국 수동으로 수행해야 합니다.
솔직하게, 프로그램에서 텍스트로 작업한다면 바이트 문자열이 아닌 일반적인 텍스트 문자열을 사용하시기 바랍니다.
